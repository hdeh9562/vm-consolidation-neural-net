{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidation with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGosssip-based virual machine consolidation in a cloud environment.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gosssip-based virual machine consolidation in a cloud environment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load depenedancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsolidationEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A datacenter has many virtual machines(VM). VMs need to get consolidated into some of the physical machines so some otheer physical machines can be turned off.\n",
    "    Source:\n",
    "        This environment corresponds to gosssip-based VM consolidation\n",
    "    Observation: \n",
    "        Type: Box(2)\n",
    "        Num\tObservation                 Min         Max\n",
    "        0\tCPU Utilization            0.0            100.0\n",
    "        1\tMemory Utilization         0.0            100.0\n",
    "        \n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num\tAction\n",
    "        0\tLow \n",
    "        1\tMedium\n",
    "        2\tHigh\n",
    "        3\txHigh\n",
    "        4\t2xHigh\n",
    "        5\t3xHigh\n",
    "        6\t4xhigh\n",
    "        7\t5xHigh\n",
    "        8\tOverload\n",
    "        \n",
    "        \n",
    "    Rewards-out:\n",
    "        In sender mode, rewards are given to a PM to move out its VMs so that it can switch off\n",
    "        Level\tReward\n",
    "        0\t1000\n",
    "        1\t900\n",
    "        2\t800\n",
    "        3\t700\n",
    "        4\t600\n",
    "        5\t500\n",
    "        6\t400\n",
    "        7\t300\n",
    "        8\t200\n",
    "        \n",
    "    Rewards-in:\n",
    "        In recipient mode, rewards are given to avoid SLA violation. It occurs when a PM moves to an overload state\n",
    "        Level\tReward\n",
    "        0\t100\n",
    "        1\t100\n",
    "        2\t100\n",
    "        3\t100\n",
    "        4\t100\n",
    "        5\t100\n",
    "        6\t100\n",
    "        7\t100\n",
    "        8\t-2000\n",
    "        \n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value between 0,100\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than ±12°\n",
    "        Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "        Episode length is greater than 200\n",
    "        Solved Requirements\n",
    "        Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second' : 50\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5 # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = 'euler'\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.cpu_min_utilization = 0.0\n",
    "        self.cpu_max_utilization = 100.0\n",
    "        self.memory_min_utilization = 0.0\n",
    "        self.memory_max_utilization = 100.0\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n",
    "        low = np.array([\n",
    "            self.cpu_min_utilization,\n",
    "            self.memory_min_utilization])\n",
    "        \n",
    "        high = np.array([\n",
    "            self.cpu_max_utilization,\n",
    "            self.memory_max_utilization])\n",
    "\n",
    "        self.action_space = spaces.Discrete(9) # actions\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32) # states\n",
    "        \n",
    "        self.reward_out = {\n",
    "            0 : 1000,\n",
    "            1 : 900,\n",
    "            2 : 800,\n",
    "            3 : 700,\n",
    "            4 : 600,\n",
    "            5 : 500,\n",
    "            6 : 400,\n",
    "            7 : 300,\n",
    "            8 : 200\n",
    "        }\n",
    "        \n",
    "        self.reward_in = {\n",
    "            0 : 100,\n",
    "            1 : 100,\n",
    "            2 : 100,\n",
    "            3 : 100,\n",
    "            4 : 100,\n",
    "            5 : 100,\n",
    "            6 : 100,\n",
    "            7 : 100,\n",
    "            8 : -2000\n",
    "        }\n",
    "        print(\"Reward {}\".format(self.reward_out))\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        state = self.state\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        force = self.force_mag if action==1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta* temp) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n",
    "        xacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x  = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else: # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x  = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "        self.state = (x,x_dot,theta,theta_dot)\n",
    "        done =  x < -self.x_threshold \\\n",
    "                or x > self.x_threshold \\\n",
    "                or theta < -self.theta_threshold_radians \\\n",
    "                or theta > self.theta_threshold_radians\n",
    "        done = bool(done)\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=0.0, high=100.0, size=(2,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward {0: 1000, 1: 900, 2: 800, 3: 700, 4: 600, 5: 500, 6: 400, 7: 300, 8: 200}\n"
     ]
    }
   ],
   "source": [
    "env = ConsolidationEnv() # initialise environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "action_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000 # n games we want agen to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'model_outpu/cartpole/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000) # double-ended queue; acts like list, but elements can be added/removed from either end\n",
    "        self.gamma = 0.95 # decay or discount rate: enables agent to take into account future actions in addition to the immediate ones, but discounted at this rate\n",
    "        self.epsilon = 1.0 # exploration rate: how much to act randomly; more initially than later due to epsilon decay\n",
    "        self.epsilon_decay = 0.995 # decrease number of random explorations as the agent's performance (hopefully) improves over time\n",
    "        self.epsilon_min = 0.01 # minimum amount of random exploration permitted\n",
    "        self.learning_rate = 0.001 # rate at which NN adjusts models parameters via SGD to reduce cost \n",
    "        self.model = self._build_model() # private method \n",
    "    \n",
    "    def _build_model(self):\n",
    "        # neural net to approximate Q-value function:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu')) # 1st hidden layer; states as input\n",
    "        model.add(Dense(24, activation='relu')) # 2nd hidden layer\n",
    "        model.add(Dense(self.action_size, activation='linear')) # 2 actions, so 2 output neurons: 0 and 1 (L/R)\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # list of previous experiences, enabling re-training later\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon: # if acting randomly, take random action\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state) # if not acting randomly, predict reward value based on current state\n",
    "        return np.argmax(act_values[0]) # pick the action that will give the highest reward (i.e., go left or right?)\n",
    "\n",
    "    def replay(self, batch_size): # method that trains NN with experiences sampled from memory\n",
    "        minibatch = random.sample(self.memory, batch_size) # sample a minibatch from memory\n",
    "        for state, action, reward, next_state, done in minibatch: # extract data for each minibatch sample\n",
    "            target = reward # if done (boolean whether game ended or not, i.e., whether final state or not), then target = reward\n",
    "            if not done: # if not done, then predict future discounted reward\n",
    "                target = (reward + self.gamma * # (target) = reward + (discount rate gamma) * \n",
    "                          np.amax(self.model.predict(next_state)[0])) # (maximum target Q based on future action a')\n",
    "            target_f = self.model.predict(state) # approximately map current state to future discounted reward\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0) # single epoch of training with x=state, y=target_f; fit decreases loss btwn target_f and y_hat\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_size, action_size) # initialise agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000\n",
      "State[CPU, Memory] => [[10.92854835 26.52157917]]\n",
      "Action:4, Reward:600\n",
      "episode: 1/1000\n",
      "State[CPU, Memory] => [[95.34512827 18.81774225]]\n",
      "Action:8, Reward:200\n",
      "episode: 2/1000\n",
      "State[CPU, Memory] => [[70.42237247  2.04040613]]\n",
      "Action:4, Reward:600\n",
      "episode: 3/1000\n",
      "State[CPU, Memory] => [[43.54217353 21.71481306]]\n",
      "Action:6, Reward:400\n",
      "episode: 4/1000\n",
      "State[CPU, Memory] => [[ 2.93043874 73.25766216]]\n",
      "Action:4, Reward:600\n",
      "episode: 5/1000\n",
      "State[CPU, Memory] => [[54.28609452 79.49538945]]\n",
      "Action:3, Reward:700\n",
      "episode: 6/1000\n",
      "State[CPU, Memory] => [[89.50862328 51.91015832]]\n",
      "Action:5, Reward:500\n",
      "episode: 7/1000\n",
      "State[CPU, Memory] => [[77.57402538 70.35001779]]\n",
      "Action:7, Reward:300\n",
      "episode: 8/1000\n",
      "State[CPU, Memory] => [[64.6195723   3.47058817]]\n",
      "Action:5, Reward:500\n",
      "episode: 9/1000\n",
      "State[CPU, Memory] => [[11.8893452  48.39524371]]\n",
      "Action:2, Reward:800\n",
      "episode: 10/1000\n",
      "State[CPU, Memory] => [[59.97204866 63.16146302]]\n",
      "Action:7, Reward:300\n",
      "episode: 11/1000\n",
      "State[CPU, Memory] => [[79.74341969 59.60034999]]\n",
      "Action:4, Reward:600\n",
      "episode: 12/1000\n",
      "State[CPU, Memory] => [[37.21869516 47.38722549]]\n",
      "Action:0, Reward:1000\n",
      "episode: 13/1000\n",
      "State[CPU, Memory] => [[10.94191362  6.62742095]]\n",
      "Action:7, Reward:300\n",
      "episode: 14/1000\n",
      "State[CPU, Memory] => [[98.36622563 97.13325651]]\n",
      "Action:2, Reward:800\n",
      "episode: 15/1000\n",
      "State[CPU, Memory] => [[57.23322036 69.79729694]]\n",
      "Action:3, Reward:700\n",
      "episode: 16/1000\n",
      "State[CPU, Memory] => [[82.71862831 97.50320881]]\n",
      "Action:7, Reward:300\n",
      "episode: 17/1000\n",
      "State[CPU, Memory] => [[78.36339582 75.85595694]]\n",
      "Action:2, Reward:800\n",
      "episode: 18/1000\n",
      "State[CPU, Memory] => [[43.44587205 31.22239954]]\n",
      "Action:5, Reward:500\n",
      "episode: 19/1000\n",
      "State[CPU, Memory] => [[89.43224772 47.44653803]]\n",
      "Action:0, Reward:1000\n",
      "episode: 20/1000\n",
      "State[CPU, Memory] => [[87.09667428 46.5072712 ]]\n",
      "Action:4, Reward:600\n",
      "episode: 21/1000\n",
      "State[CPU, Memory] => [[83.50885042 65.16697766]]\n",
      "Action:0, Reward:1000\n",
      "episode: 22/1000\n",
      "State[CPU, Memory] => [[40.37139826 22.24487175]]\n",
      "Action:4, Reward:600\n",
      "episode: 23/1000\n",
      "State[CPU, Memory] => [[62.18116242 88.04081112]]\n",
      "Action:5, Reward:500\n",
      "episode: 24/1000\n",
      "State[CPU, Memory] => [[69.7931975  8.7754721]]\n",
      "Action:1, Reward:900\n",
      "episode: 25/1000\n",
      "State[CPU, Memory] => [[43.81089963 26.81226165]]\n",
      "Action:3, Reward:700\n",
      "episode: 26/1000\n",
      "State[CPU, Memory] => [[84.91841183 24.76843382]]\n",
      "Action:8, Reward:200\n",
      "episode: 27/1000\n",
      "State[CPU, Memory] => [[13.7749131  20.67386263]]\n",
      "Action:8, Reward:200\n",
      "episode: 28/1000\n",
      "State[CPU, Memory] => [[35.52622165 81.25204549]]\n",
      "Action:2, Reward:800\n",
      "episode: 29/1000\n",
      "State[CPU, Memory] => [[80.06220672 99.13141269]]\n",
      "Action:0, Reward:1000\n",
      "episode: 30/1000\n",
      "State[CPU, Memory] => [[36.72621894 72.39874614]]\n",
      "Action:0, Reward:1000\n",
      "episode: 31/1000\n",
      "State[CPU, Memory] => [[54.31581511 18.86120457]]\n",
      "Action:6, Reward:400\n",
      "episode: 32/1000\n",
      "State[CPU, Memory] => [[53.44469864 71.09897136]]\n",
      "Action:0, Reward:1000\n",
      "episode: 33/1000\n",
      "State[CPU, Memory] => [[57.31363151 49.88222387]]\n",
      "Action:8, Reward:200\n",
      "episode: 34/1000\n",
      "State[CPU, Memory] => [[28.82250274 72.52200359]]\n",
      "Action:7, Reward:300\n",
      "episode: 35/1000\n",
      "State[CPU, Memory] => [[23.94591664  1.26724663]]\n",
      "Action:8, Reward:200\n",
      "episode: 36/1000\n",
      "State[CPU, Memory] => [[37.4837241 80.975349 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 37/1000\n",
      "State[CPU, Memory] => [[68.05766003 37.5226663 ]]\n",
      "Action:8, Reward:200\n",
      "episode: 38/1000\n",
      "State[CPU, Memory] => [[20.10310429 72.9062605 ]]\n",
      "Action:2, Reward:800\n",
      "episode: 39/1000\n",
      "State[CPU, Memory] => [[85.03189012 86.10456172]]\n",
      "Action:7, Reward:300\n",
      "episode: 40/1000\n",
      "State[CPU, Memory] => [[38.12365805 79.82121224]]\n",
      "Action:4, Reward:600\n",
      "episode: 41/1000\n",
      "State[CPU, Memory] => [[33.60046787 11.59940459]]\n",
      "Action:1, Reward:900\n",
      "episode: 42/1000\n",
      "State[CPU, Memory] => [[ 5.32240567 90.98477221]]\n",
      "Action:6, Reward:400\n",
      "episode: 43/1000\n",
      "State[CPU, Memory] => [[14.1930868  11.84169084]]\n",
      "Action:1, Reward:900\n",
      "episode: 44/1000\n",
      "State[CPU, Memory] => [[29.8556098  98.08715296]]\n",
      "Action:6, Reward:400\n",
      "episode: 45/1000\n",
      "State[CPU, Memory] => [[20.85850133 13.21790367]]\n",
      "Action:1, Reward:900\n",
      "episode: 46/1000\n",
      "State[CPU, Memory] => [[70.08031361 54.65722633]]\n",
      "Action:7, Reward:300\n",
      "episode: 47/1000\n",
      "State[CPU, Memory] => [[84.77736176 80.0575318 ]]\n",
      "Action:8, Reward:200\n",
      "episode: 48/1000\n",
      "State[CPU, Memory] => [[70.94838034 11.90326334]]\n",
      "Action:7, Reward:300\n",
      "episode: 49/1000\n",
      "State[CPU, Memory] => [[62.22950414 41.20345446]]\n",
      "Action:1, Reward:900\n",
      "episode: 50/1000\n",
      "State[CPU, Memory] => [[74.33365172 12.93671625]]\n",
      "Action:8, Reward:200\n",
      "episode: 51/1000\n",
      "State[CPU, Memory] => [[20.23262548 55.47227501]]\n",
      "Action:3, Reward:700\n",
      "episode: 52/1000\n",
      "State[CPU, Memory] => [[92.20926081 28.34546916]]\n",
      "Action:2, Reward:800\n",
      "episode: 53/1000\n",
      "State[CPU, Memory] => [[ 5.23221087 52.51637478]]\n",
      "Action:4, Reward:600\n",
      "episode: 54/1000\n",
      "State[CPU, Memory] => [[37.19517664 68.26861818]]\n",
      "Action:0, Reward:1000\n",
      "episode: 55/1000\n",
      "State[CPU, Memory] => [[24.14516458 38.19348855]]\n",
      "Action:5, Reward:500\n",
      "episode: 56/1000\n",
      "State[CPU, Memory] => [[31.84686292 70.97221798]]\n",
      "Action:1, Reward:900\n",
      "episode: 57/1000\n",
      "State[CPU, Memory] => [[35.55471598 44.3941376 ]]\n",
      "Action:7, Reward:300\n",
      "episode: 58/1000\n",
      "State[CPU, Memory] => [[15.56158673 85.76040779]]\n",
      "Action:6, Reward:400\n",
      "episode: 59/1000\n",
      "State[CPU, Memory] => [[69.18838753 35.47441461]]\n",
      "Action:6, Reward:400\n",
      "episode: 60/1000\n",
      "State[CPU, Memory] => [[59.56542347 58.45043789]]\n",
      "Action:6, Reward:400\n",
      "episode: 61/1000\n",
      "State[CPU, Memory] => [[97.22735772 11.76088742]]\n",
      "Action:5, Reward:500\n",
      "episode: 62/1000\n",
      "State[CPU, Memory] => [[43.97140417 80.16965478]]\n",
      "Action:7, Reward:300\n",
      "episode: 63/1000\n",
      "State[CPU, Memory] => [[84.6457448  29.76429923]]\n",
      "Action:1, Reward:900\n",
      "episode: 64/1000\n",
      "State[CPU, Memory] => [[12.82542422 16.18132312]]\n",
      "Action:5, Reward:500\n",
      "episode: 65/1000\n",
      "State[CPU, Memory] => [[38.20002203 77.98138654]]\n",
      "Action:0, Reward:1000\n",
      "episode: 66/1000\n",
      "State[CPU, Memory] => [[97.48615956 10.3995797 ]]\n",
      "Action:7, Reward:300\n",
      "episode: 67/1000\n",
      "State[CPU, Memory] => [[90.62726384 54.07731781]]\n",
      "Action:4, Reward:600\n",
      "episode: 68/1000\n",
      "State[CPU, Memory] => [[83.24737848 33.35658263]]\n",
      "Action:1, Reward:900\n",
      "episode: 69/1000\n",
      "State[CPU, Memory] => [[16.32835569 17.96130142]]\n",
      "Action:5, Reward:500\n",
      "episode: 70/1000\n",
      "State[CPU, Memory] => [[72.28273531 54.42759541]]\n",
      "Action:4, Reward:600\n",
      "episode: 71/1000\n",
      "State[CPU, Memory] => [[15.10574495 97.8162419 ]]\n",
      "Action:3, Reward:700\n",
      "episode: 72/1000\n",
      "State[CPU, Memory] => [[49.83936714 76.08347557]]\n",
      "Action:7, Reward:300\n",
      "episode: 73/1000\n",
      "State[CPU, Memory] => [[56.1204178  40.49997166]]\n",
      "Action:8, Reward:200\n",
      "episode: 74/1000\n",
      "State[CPU, Memory] => [[72.04924424 39.32970978]]\n",
      "Action:7, Reward:300\n",
      "episode: 75/1000\n",
      "State[CPU, Memory] => [[46.02591113  9.3866016 ]]\n",
      "Action:3, Reward:700\n",
      "episode: 76/1000\n",
      "State[CPU, Memory] => [[85.71193988 60.02021195]]\n",
      "Action:4, Reward:600\n",
      "episode: 77/1000\n",
      "State[CPU, Memory] => [[15.1567133   8.57063699]]\n",
      "Action:8, Reward:200\n",
      "episode: 78/1000\n",
      "State[CPU, Memory] => [[92.01293538 19.05527487]]\n",
      "Action:6, Reward:400\n",
      "episode: 79/1000\n",
      "State[CPU, Memory] => [[47.91075274 13.40541219]]\n",
      "Action:7, Reward:300\n",
      "episode: 80/1000\n",
      "State[CPU, Memory] => [[64.47341237 33.27376619]]\n",
      "Action:7, Reward:300\n",
      "episode: 81/1000\n",
      "State[CPU, Memory] => [[11.85572379 67.45022331]]\n",
      "Action:1, Reward:900\n",
      "episode: 82/1000\n",
      "State[CPU, Memory] => [[59.55726355  2.59792685]]\n",
      "Action:1, Reward:900\n",
      "episode: 83/1000\n",
      "State[CPU, Memory] => [[79.12245504  4.77049272]]\n",
      "Action:5, Reward:500\n",
      "episode: 84/1000\n",
      "State[CPU, Memory] => [[73.63624413 65.07661454]]\n",
      "Action:4, Reward:600\n",
      "episode: 85/1000\n",
      "State[CPU, Memory] => [[27.15430562 40.39149976]]\n",
      "Action:7, Reward:300\n",
      "episode: 86/1000\n",
      "State[CPU, Memory] => [[93.4803709  32.91698671]]\n",
      "Action:8, Reward:200\n",
      "episode: 87/1000\n",
      "State[CPU, Memory] => [[34.02603302 73.37568337]]\n",
      "Action:8, Reward:200\n",
      "episode: 88/1000\n",
      "State[CPU, Memory] => [[25.98639586 14.6733601 ]]\n",
      "Action:6, Reward:400\n",
      "episode: 89/1000\n",
      "State[CPU, Memory] => [[45.08753524 64.80261709]]\n",
      "Action:6, Reward:400\n",
      "episode: 90/1000\n",
      "State[CPU, Memory] => [[77.54209423 25.51327015]]\n",
      "Action:1, Reward:900\n",
      "episode: 91/1000\n",
      "State[CPU, Memory] => [[26.42530233 15.15567552]]\n",
      "Action:8, Reward:200\n",
      "episode: 92/1000\n",
      "State[CPU, Memory] => [[55.80575396 48.35900222]]\n",
      "Action:5, Reward:500\n",
      "episode: 93/1000\n",
      "State[CPU, Memory] => [[45.07014997 21.64514459]]\n",
      "Action:8, Reward:200\n",
      "episode: 94/1000\n",
      "State[CPU, Memory] => [[84.06909913 43.14291711]]\n",
      "Action:7, Reward:300\n",
      "episode: 95/1000\n",
      "State[CPU, Memory] => [[53.91769873 71.69431192]]\n",
      "Action:1, Reward:900\n",
      "episode: 96/1000\n",
      "State[CPU, Memory] => [[42.08421127 64.71391083]]\n",
      "Action:5, Reward:500\n",
      "episode: 97/1000\n",
      "State[CPU, Memory] => [[32.52056572 66.00432195]]\n",
      "Action:8, Reward:200\n",
      "episode: 98/1000\n",
      "State[CPU, Memory] => [[27.00615751 85.09604271]]\n",
      "Action:3, Reward:700\n",
      "episode: 99/1000\n",
      "State[CPU, Memory] => [[9.32735507 1.44872615]]\n",
      "Action:6, Reward:400\n",
      "episode: 100/1000\n",
      "State[CPU, Memory] => [[30.23177135 91.37472487]]\n",
      "Action:1, Reward:900\n",
      "episode: 101/1000\n",
      "State[CPU, Memory] => [[52.74867823 58.79650539]]\n",
      "Action:0, Reward:1000\n",
      "episode: 102/1000\n",
      "State[CPU, Memory] => [[56.84685895 39.3988766 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 103/1000\n",
      "State[CPU, Memory] => [[58.02181409 87.64949233]]\n",
      "Action:8, Reward:200\n",
      "episode: 104/1000\n",
      "State[CPU, Memory] => [[4.02016099 6.47041017]]\n",
      "Action:8, Reward:200\n",
      "episode: 105/1000\n",
      "State[CPU, Memory] => [[57.30456674 82.24907991]]\n",
      "Action:5, Reward:500\n",
      "episode: 106/1000\n",
      "State[CPU, Memory] => [[17.7342785  57.61232752]]\n",
      "Action:6, Reward:400\n",
      "episode: 107/1000\n",
      "State[CPU, Memory] => [[75.86219267 39.40624829]]\n",
      "Action:2, Reward:800\n",
      "episode: 108/1000\n",
      "State[CPU, Memory] => [[79.8818005  40.62634839]]\n",
      "Action:8, Reward:200\n",
      "episode: 109/1000\n",
      "State[CPU, Memory] => [[21.87091262 34.79760019]]\n",
      "Action:4, Reward:600\n",
      "episode: 110/1000\n",
      "State[CPU, Memory] => [[ 3.74307498 77.40144975]]\n",
      "Action:3, Reward:700\n",
      "episode: 111/1000\n",
      "State[CPU, Memory] => [[79.91876682 12.64949189]]\n",
      "Action:2, Reward:800\n",
      "episode: 112/1000\n",
      "State[CPU, Memory] => [[81.55070528 22.34981621]]\n",
      "Action:1, Reward:900\n",
      "episode: 113/1000\n",
      "State[CPU, Memory] => [[ 2.30465476 21.00447847]]\n",
      "Action:3, Reward:700\n",
      "episode: 114/1000\n",
      "State[CPU, Memory] => [[22.67441544  4.13333813]]\n",
      "Action:8, Reward:200\n",
      "episode: 115/1000\n",
      "State[CPU, Memory] => [[76.89446504 93.49252296]]\n",
      "Action:4, Reward:600\n",
      "episode: 116/1000\n",
      "State[CPU, Memory] => [[ 3.01606748 80.51340445]]\n",
      "Action:3, Reward:700\n",
      "episode: 117/1000\n",
      "State[CPU, Memory] => [[52.30163723 20.30028321]]\n",
      "Action:6, Reward:400\n",
      "episode: 118/1000\n",
      "State[CPU, Memory] => [[19.16683397 74.00930845]]\n",
      "Action:2, Reward:800\n",
      "episode: 119/1000\n",
      "State[CPU, Memory] => [[44.3827438   9.04121989]]\n",
      "Action:3, Reward:700\n",
      "episode: 120/1000\n",
      "State[CPU, Memory] => [[83.43972734 21.69227818]]\n",
      "Action:1, Reward:900\n",
      "episode: 121/1000\n",
      "State[CPU, Memory] => [[87.93820508  1.68768148]]\n",
      "Action:3, Reward:700\n",
      "episode: 122/1000\n",
      "State[CPU, Memory] => [[56.03967659 47.78021981]]\n",
      "Action:0, Reward:1000\n",
      "episode: 123/1000\n",
      "State[CPU, Memory] => [[24.76400667 94.78945176]]\n",
      "Action:3, Reward:700\n",
      "episode: 124/1000\n",
      "State[CPU, Memory] => [[ 1.74300601 16.77535584]]\n",
      "Action:1, Reward:900\n",
      "episode: 125/1000\n",
      "State[CPU, Memory] => [[32.54125324 43.9195092 ]]\n",
      "Action:8, Reward:200\n",
      "episode: 126/1000\n",
      "State[CPU, Memory] => [[22.25721932 85.09362115]]\n",
      "Action:6, Reward:400\n",
      "episode: 127/1000\n",
      "State[CPU, Memory] => [[18.38267858 46.48405011]]\n",
      "Action:2, Reward:800\n",
      "episode: 128/1000\n",
      "State[CPU, Memory] => [[70.81141052 86.11653221]]\n",
      "Action:7, Reward:300\n",
      "episode: 129/1000\n",
      "State[CPU, Memory] => [[10.23953079 28.03020767]]\n",
      "Action:8, Reward:200\n",
      "episode: 130/1000\n",
      "State[CPU, Memory] => [[25.64724779 36.58777714]]\n",
      "Action:0, Reward:1000\n",
      "episode: 131/1000\n",
      "State[CPU, Memory] => [[72.7074049   6.27704419]]\n",
      "Action:7, Reward:300\n",
      "episode: 132/1000\n",
      "State[CPU, Memory] => [[31.36711603 50.79271221]]\n",
      "Action:5, Reward:500\n",
      "episode: 133/1000\n",
      "State[CPU, Memory] => [[32.87948209 65.18692828]]\n",
      "Action:5, Reward:500\n",
      "episode: 134/1000\n",
      "State[CPU, Memory] => [[31.31946566 40.88414981]]\n",
      "Action:2, Reward:800\n",
      "episode: 135/1000\n",
      "State[CPU, Memory] => [[66.93812754 11.00822549]]\n",
      "Action:5, Reward:500\n",
      "episode: 136/1000\n",
      "State[CPU, Memory] => [[23.51278769 78.37591809]]\n",
      "Action:6, Reward:400\n",
      "episode: 137/1000\n",
      "State[CPU, Memory] => [[38.05312088 14.46204759]]\n",
      "Action:7, Reward:300\n",
      "episode: 138/1000\n",
      "State[CPU, Memory] => [[88.23651455  8.06723784]]\n",
      "Action:5, Reward:500\n",
      "episode: 139/1000\n",
      "State[CPU, Memory] => [[77.42721911  4.58308477]]\n",
      "Action:7, Reward:300\n",
      "episode: 140/1000\n",
      "State[CPU, Memory] => [[45.42837375 51.99833461]]\n",
      "Action:6, Reward:400\n",
      "episode: 141/1000\n",
      "State[CPU, Memory] => [[ 2.70266844 47.08415582]]\n",
      "Action:6, Reward:400\n",
      "episode: 142/1000\n",
      "State[CPU, Memory] => [[56.91839276 74.08713757]]\n",
      "Action:4, Reward:600\n",
      "episode: 143/1000\n",
      "State[CPU, Memory] => [[42.65610001 47.86748771]]\n",
      "Action:0, Reward:1000\n",
      "episode: 144/1000\n",
      "State[CPU, Memory] => [[ 7.08796576 76.01163472]]\n",
      "Action:7, Reward:300\n",
      "episode: 145/1000\n",
      "State[CPU, Memory] => [[6.07891837 3.27294526]]\n",
      "Action:2, Reward:800\n",
      "episode: 146/1000\n",
      "State[CPU, Memory] => [[21.24452269 76.42079966]]\n",
      "Action:7, Reward:300\n",
      "episode: 147/1000\n",
      "State[CPU, Memory] => [[49.9074098  89.49895023]]\n",
      "Action:3, Reward:700\n",
      "episode: 148/1000\n",
      "State[CPU, Memory] => [[95.17031596 80.74097152]]\n",
      "Action:5, Reward:500\n",
      "episode: 149/1000\n",
      "State[CPU, Memory] => [[44.66504606 96.80868105]]\n",
      "Action:4, Reward:600\n",
      "episode: 150/1000\n",
      "State[CPU, Memory] => [[11.21231825 64.98623284]]\n",
      "Action:2, Reward:800\n",
      "episode: 151/1000\n",
      "State[CPU, Memory] => [[28.93082871 61.44172457]]\n",
      "Action:0, Reward:1000\n",
      "episode: 152/1000\n",
      "State[CPU, Memory] => [[86.95038499 36.76648932]]\n",
      "Action:4, Reward:600\n",
      "episode: 153/1000\n",
      "State[CPU, Memory] => [[ 4.35363694 60.49019847]]\n",
      "Action:4, Reward:600\n",
      "episode: 154/1000\n",
      "State[CPU, Memory] => [[53.43496506 44.59457957]]\n",
      "Action:8, Reward:200\n",
      "episode: 155/1000\n",
      "State[CPU, Memory] => [[13.30726964 62.20075741]]\n",
      "Action:5, Reward:500\n",
      "episode: 156/1000\n",
      "State[CPU, Memory] => [[62.9390918  24.16187094]]\n",
      "Action:5, Reward:500\n",
      "episode: 157/1000\n",
      "State[CPU, Memory] => [[95.55289802 96.58208613]]\n",
      "Action:2, Reward:800\n",
      "episode: 158/1000\n",
      "State[CPU, Memory] => [[58.52989264  5.75333309]]\n",
      "Action:3, Reward:700\n",
      "episode: 159/1000\n",
      "State[CPU, Memory] => [[10.37798888 10.92753854]]\n",
      "Action:3, Reward:700\n",
      "episode: 160/1000\n",
      "State[CPU, Memory] => [[61.02610333 61.32689264]]\n",
      "Action:4, Reward:600\n",
      "episode: 161/1000\n",
      "State[CPU, Memory] => [[32.57798346 74.5395837 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 162/1000\n",
      "State[CPU, Memory] => [[14.14259367 42.80488963]]\n",
      "Action:7, Reward:300\n",
      "episode: 163/1000\n",
      "State[CPU, Memory] => [[13.60665807 86.46216652]]\n",
      "Action:0, Reward:1000\n",
      "episode: 164/1000\n",
      "State[CPU, Memory] => [[12.17192865 84.69596176]]\n",
      "Action:2, Reward:800\n",
      "episode: 165/1000\n",
      "State[CPU, Memory] => [[68.88221045  8.81118024]]\n",
      "Action:0, Reward:1000\n",
      "episode: 166/1000\n",
      "State[CPU, Memory] => [[42.13810503 27.78993402]]\n",
      "Action:4, Reward:600\n",
      "episode: 167/1000\n",
      "State[CPU, Memory] => [[48.49463318 44.5786842 ]]\n",
      "Action:3, Reward:700\n",
      "episode: 168/1000\n",
      "State[CPU, Memory] => [[ 5.33874456 80.89888798]]\n",
      "Action:1, Reward:900\n",
      "episode: 169/1000\n",
      "State[CPU, Memory] => [[94.09613099 76.35719297]]\n",
      "Action:6, Reward:400\n",
      "episode: 170/1000\n",
      "State[CPU, Memory] => [[69.49518315 32.98942456]]\n",
      "Action:1, Reward:900\n",
      "episode: 171/1000\n",
      "State[CPU, Memory] => [[99.88627103 55.01714596]]\n",
      "Action:8, Reward:200\n",
      "episode: 172/1000\n",
      "State[CPU, Memory] => [[64.99321955 55.86526141]]\n",
      "Action:1, Reward:900\n",
      "episode: 173/1000\n",
      "State[CPU, Memory] => [[35.31412416 87.63658608]]\n",
      "Action:8, Reward:200\n",
      "episode: 174/1000\n",
      "State[CPU, Memory] => [[ 9.63794592 35.34143623]]\n",
      "Action:0, Reward:1000\n",
      "episode: 175/1000\n",
      "State[CPU, Memory] => [[38.13102365 89.00694743]]\n",
      "Action:6, Reward:400\n",
      "episode: 176/1000\n",
      "State[CPU, Memory] => [[26.83137653 85.64652175]]\n",
      "Action:8, Reward:200\n",
      "episode: 177/1000\n",
      "State[CPU, Memory] => [[37.33625725 11.58929152]]\n",
      "Action:8, Reward:200\n",
      "episode: 178/1000\n",
      "State[CPU, Memory] => [[22.9248618 55.8722924]]\n",
      "Action:1, Reward:900\n",
      "episode: 179/1000\n",
      "State[CPU, Memory] => [[96.96118932 93.16498945]]\n",
      "Action:5, Reward:500\n",
      "episode: 180/1000\n",
      "State[CPU, Memory] => [[82.84079331 74.20134716]]\n",
      "Action:7, Reward:300\n",
      "episode: 181/1000\n",
      "State[CPU, Memory] => [[83.69049915 66.51034788]]\n",
      "Action:4, Reward:600\n",
      "episode: 182/1000\n",
      "State[CPU, Memory] => [[30.81343769 22.00841195]]\n",
      "Action:0, Reward:1000\n",
      "episode: 183/1000\n",
      "State[CPU, Memory] => [[20.59880231 26.35980455]]\n",
      "Action:5, Reward:500\n",
      "episode: 184/1000\n",
      "State[CPU, Memory] => [[90.51913871 28.44725678]]\n",
      "Action:8, Reward:200\n",
      "episode: 185/1000\n",
      "State[CPU, Memory] => [[45.51663686 51.03032663]]\n",
      "Action:2, Reward:800\n",
      "episode: 186/1000\n",
      "State[CPU, Memory] => [[99.31475185 95.17137296]]\n",
      "Action:8, Reward:200\n",
      "episode: 187/1000\n",
      "State[CPU, Memory] => [[46.69265176 77.09259878]]\n",
      "Action:6, Reward:400\n",
      "episode: 188/1000\n",
      "State[CPU, Memory] => [[26.8620632  83.15338844]]\n",
      "Action:1, Reward:900\n",
      "episode: 189/1000\n",
      "State[CPU, Memory] => [[ 1.1399598 66.2520339]]\n",
      "Action:3, Reward:700\n",
      "episode: 190/1000\n",
      "State[CPU, Memory] => [[53.4903015  39.43476827]]\n",
      "Action:4, Reward:600\n",
      "episode: 191/1000\n",
      "State[CPU, Memory] => [[ 7.26126888 24.23804725]]\n",
      "Action:6, Reward:400\n",
      "episode: 192/1000\n",
      "State[CPU, Memory] => [[70.45769076 82.5521371 ]]\n",
      "Action:0, Reward:1000\n",
      "episode: 193/1000\n",
      "State[CPU, Memory] => [[61.41895655  2.527228  ]]\n",
      "Action:2, Reward:800\n",
      "episode: 194/1000\n",
      "State[CPU, Memory] => [[77.4722357  23.64087614]]\n",
      "Action:0, Reward:1000\n",
      "episode: 195/1000\n",
      "State[CPU, Memory] => [[87.79550393 45.73734624]]\n",
      "Action:5, Reward:500\n",
      "episode: 196/1000\n",
      "State[CPU, Memory] => [[61.43483239 94.90218604]]\n",
      "Action:5, Reward:500\n",
      "episode: 197/1000\n",
      "State[CPU, Memory] => [[86.07327255  8.60472825]]\n",
      "Action:8, Reward:200\n",
      "episode: 198/1000\n",
      "State[CPU, Memory] => [[20.36034054 44.36608968]]\n",
      "Action:0, Reward:1000\n",
      "episode: 199/1000\n",
      "State[CPU, Memory] => [[49.64848632 76.4074938 ]]\n",
      "Action:4, Reward:600\n",
      "episode: 200/1000\n",
      "State[CPU, Memory] => [[96.13963364 73.6248427 ]]\n",
      "Action:7, Reward:300\n",
      "episode: 201/1000\n",
      "State[CPU, Memory] => [[85.50666247 27.42959239]]\n",
      "Action:0, Reward:1000\n",
      "episode: 202/1000\n",
      "State[CPU, Memory] => [[51.81173245  9.56239835]]\n",
      "Action:7, Reward:300\n",
      "episode: 203/1000\n",
      "State[CPU, Memory] => [[70.99276764 73.3456968 ]]\n",
      "Action:7, Reward:300\n",
      "episode: 204/1000\n",
      "State[CPU, Memory] => [[47.39200505  8.07288486]]\n",
      "Action:3, Reward:700\n",
      "episode: 205/1000\n",
      "State[CPU, Memory] => [[46.25459441  2.19104534]]\n",
      "Action:7, Reward:300\n",
      "episode: 206/1000\n",
      "State[CPU, Memory] => [[49.08424187 89.83422661]]\n",
      "Action:4, Reward:600\n",
      "episode: 207/1000\n",
      "State[CPU, Memory] => [[66.82310638 44.45624194]]\n",
      "Action:6, Reward:400\n",
      "episode: 208/1000\n",
      "State[CPU, Memory] => [[45.92409809 90.33499253]]\n",
      "Action:0, Reward:1000\n",
      "episode: 209/1000\n",
      "State[CPU, Memory] => [[11.59365014  3.07653636]]\n",
      "Action:3, Reward:700\n",
      "episode: 210/1000\n",
      "State[CPU, Memory] => [[53.74244216 42.86936247]]\n",
      "Action:2, Reward:800\n",
      "episode: 211/1000\n",
      "State[CPU, Memory] => [[37.8226014 25.6665575]]\n",
      "Action:5, Reward:500\n",
      "episode: 212/1000\n",
      "State[CPU, Memory] => [[49.38357937 17.45503783]]\n",
      "Action:7, Reward:300\n",
      "episode: 213/1000\n",
      "State[CPU, Memory] => [[21.61840253 77.58598332]]\n",
      "Action:7, Reward:300\n",
      "episode: 214/1000\n",
      "State[CPU, Memory] => [[32.66828057 26.19574293]]\n",
      "Action:3, Reward:700\n",
      "episode: 215/1000\n",
      "State[CPU, Memory] => [[81.51466643 13.68206732]]\n",
      "Action:1, Reward:900\n",
      "episode: 216/1000\n",
      "State[CPU, Memory] => [[80.15819476 24.33875454]]\n",
      "Action:0, Reward:1000\n",
      "episode: 217/1000\n",
      "State[CPU, Memory] => [[63.37240937 85.3833327 ]]\n",
      "Action:5, Reward:500\n",
      "episode: 218/1000\n",
      "State[CPU, Memory] => [[35.68818188 31.94284678]]\n",
      "Action:0, Reward:1000\n",
      "episode: 219/1000\n",
      "State[CPU, Memory] => [[90.03893946 92.76755736]]\n",
      "Action:2, Reward:800\n",
      "episode: 220/1000\n",
      "State[CPU, Memory] => [[35.02005369 43.9751205 ]]\n",
      "Action:2, Reward:800\n",
      "episode: 221/1000\n",
      "State[CPU, Memory] => [[71.92045123 11.5359416 ]]\n",
      "Action:5, Reward:500\n",
      "episode: 222/1000\n",
      "State[CPU, Memory] => [[93.80485037 49.99248502]]\n",
      "Action:6, Reward:400\n",
      "episode: 223/1000\n",
      "State[CPU, Memory] => [[87.94069643 37.9487562 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 224/1000\n",
      "State[CPU, Memory] => [[12.30971135 94.37182802]]\n",
      "Action:7, Reward:300\n",
      "episode: 225/1000\n",
      "State[CPU, Memory] => [[25.2204669  36.40267323]]\n",
      "Action:2, Reward:800\n",
      "episode: 226/1000\n",
      "State[CPU, Memory] => [[99.67910473  7.99474665]]\n",
      "Action:7, Reward:300\n",
      "episode: 227/1000\n",
      "State[CPU, Memory] => [[83.94292683  6.68105223]]\n",
      "Action:1, Reward:900\n",
      "episode: 228/1000\n",
      "State[CPU, Memory] => [[49.77517835 36.35885707]]\n",
      "Action:2, Reward:800\n",
      "episode: 229/1000\n",
      "State[CPU, Memory] => [[12.78917115 17.44730753]]\n",
      "Action:0, Reward:1000\n",
      "episode: 230/1000\n",
      "State[CPU, Memory] => [[44.13371121 84.39130239]]\n",
      "Action:4, Reward:600\n",
      "episode: 231/1000\n",
      "State[CPU, Memory] => [[68.062584   56.11317244]]\n",
      "Action:2, Reward:800\n",
      "episode: 232/1000\n",
      "State[CPU, Memory] => [[58.62871311  9.97470571]]\n",
      "Action:2, Reward:800\n",
      "episode: 233/1000\n",
      "State[CPU, Memory] => [[95.25045433 16.66497691]]\n",
      "Action:7, Reward:300\n",
      "episode: 234/1000\n",
      "State[CPU, Memory] => [[94.57558112 54.11638752]]\n",
      "Action:2, Reward:800\n",
      "episode: 235/1000\n",
      "State[CPU, Memory] => [[66.98628678 86.04643636]]\n",
      "Action:3, Reward:700\n",
      "episode: 236/1000\n",
      "State[CPU, Memory] => [[92.70709656 50.70595805]]\n",
      "Action:8, Reward:200\n",
      "episode: 237/1000\n",
      "State[CPU, Memory] => [[37.62068562 30.83894835]]\n",
      "Action:7, Reward:300\n",
      "episode: 238/1000\n",
      "State[CPU, Memory] => [[69.07427376 61.66076969]]\n",
      "Action:1, Reward:900\n",
      "episode: 239/1000\n",
      "State[CPU, Memory] => [[20.83645855 75.98025856]]\n",
      "Action:3, Reward:700\n",
      "episode: 240/1000\n",
      "State[CPU, Memory] => [[40.97053608 55.58269689]]\n",
      "Action:0, Reward:1000\n",
      "episode: 241/1000\n",
      "State[CPU, Memory] => [[89.19243676 98.34644814]]\n",
      "Action:1, Reward:900\n",
      "episode: 242/1000\n",
      "State[CPU, Memory] => [[57.16743151 30.66168398]]\n",
      "Action:6, Reward:400\n",
      "episode: 243/1000\n",
      "State[CPU, Memory] => [[96.09532456 72.19003503]]\n",
      "Action:5, Reward:500\n",
      "episode: 244/1000\n",
      "State[CPU, Memory] => [[92.38343983 95.79436475]]\n",
      "Action:5, Reward:500\n",
      "episode: 245/1000\n",
      "State[CPU, Memory] => [[83.13323142 28.15703569]]\n",
      "Action:8, Reward:200\n",
      "episode: 246/1000\n",
      "State[CPU, Memory] => [[72.25604115 83.3662704 ]]\n",
      "Action:3, Reward:700\n",
      "episode: 247/1000\n",
      "State[CPU, Memory] => [[7.7504698  2.32295176]]\n",
      "Action:1, Reward:900\n",
      "episode: 248/1000\n",
      "State[CPU, Memory] => [[18.11867394 45.19627039]]\n",
      "Action:0, Reward:1000\n",
      "episode: 249/1000\n",
      "State[CPU, Memory] => [[11.49329347 18.16444189]]\n",
      "Action:5, Reward:500\n",
      "episode: 250/1000\n",
      "State[CPU, Memory] => [[68.41954843 59.26871469]]\n",
      "Action:3, Reward:700\n",
      "episode: 251/1000\n",
      "State[CPU, Memory] => [[52.08910135 25.59785062]]\n",
      "Action:8, Reward:200\n",
      "episode: 252/1000\n",
      "State[CPU, Memory] => [[49.72399477 61.06597909]]\n",
      "Action:2, Reward:800\n",
      "episode: 253/1000\n",
      "State[CPU, Memory] => [[33.50549187 69.04367568]]\n",
      "Action:8, Reward:200\n",
      "episode: 254/1000\n",
      "State[CPU, Memory] => [[64.21598492 46.08613732]]\n",
      "Action:6, Reward:400\n",
      "episode: 255/1000\n",
      "State[CPU, Memory] => [[68.95192996 56.42562368]]\n",
      "Action:5, Reward:500\n",
      "episode: 256/1000\n",
      "State[CPU, Memory] => [[37.32126695 99.3043213 ]]\n",
      "Action:2, Reward:800\n",
      "episode: 257/1000\n",
      "State[CPU, Memory] => [[65.27748685  8.22689323]]\n",
      "Action:3, Reward:700\n",
      "episode: 258/1000\n",
      "State[CPU, Memory] => [[26.75780892 51.73346397]]\n",
      "Action:3, Reward:700\n",
      "episode: 259/1000\n",
      "State[CPU, Memory] => [[90.4909622  54.41046218]]\n",
      "Action:0, Reward:1000\n",
      "episode: 260/1000\n",
      "State[CPU, Memory] => [[ 6.88385162 63.63492101]]\n",
      "Action:5, Reward:500\n",
      "episode: 261/1000\n",
      "State[CPU, Memory] => [[37.91955018 63.88519124]]\n",
      "Action:6, Reward:400\n",
      "episode: 262/1000\n",
      "State[CPU, Memory] => [[18.61621977 38.74680178]]\n",
      "Action:0, Reward:1000\n",
      "episode: 263/1000\n",
      "State[CPU, Memory] => [[27.73211012 49.95967962]]\n",
      "Action:5, Reward:500\n",
      "episode: 264/1000\n",
      "State[CPU, Memory] => [[49.61298992 18.06474805]]\n",
      "Action:7, Reward:300\n",
      "episode: 265/1000\n",
      "State[CPU, Memory] => [[15.70733156 41.00695966]]\n",
      "Action:6, Reward:400\n",
      "episode: 266/1000\n",
      "State[CPU, Memory] => [[93.71088406 77.07641235]]\n",
      "Action:1, Reward:900\n",
      "episode: 267/1000\n",
      "State[CPU, Memory] => [[42.78181185  5.35592388]]\n",
      "Action:4, Reward:600\n",
      "episode: 268/1000\n",
      "State[CPU, Memory] => [[73.72650115 61.32553088]]\n",
      "Action:1, Reward:900\n",
      "episode: 269/1000\n",
      "State[CPU, Memory] => [[86.88357528  8.43786399]]\n",
      "Action:6, Reward:400\n",
      "episode: 270/1000\n",
      "State[CPU, Memory] => [[69.98924639 51.1825336 ]]\n",
      "Action:5, Reward:500\n",
      "episode: 271/1000\n",
      "State[CPU, Memory] => [[71.22433918 61.03024627]]\n",
      "Action:1, Reward:900\n",
      "episode: 272/1000\n",
      "State[CPU, Memory] => [[17.0271712  37.29650934]]\n",
      "Action:4, Reward:600\n",
      "episode: 273/1000\n",
      "State[CPU, Memory] => [[64.08202798 96.17712841]]\n",
      "Action:6, Reward:400\n",
      "episode: 274/1000\n",
      "State[CPU, Memory] => [[18.91579985 62.47526166]]\n",
      "Action:4, Reward:600\n",
      "episode: 275/1000\n",
      "State[CPU, Memory] => [[84.66189043  1.29818712]]\n",
      "Action:3, Reward:700\n",
      "episode: 276/1000\n",
      "State[CPU, Memory] => [[59.5903548   2.45420444]]\n",
      "Action:1, Reward:900\n",
      "episode: 277/1000\n",
      "State[CPU, Memory] => [[26.46678986 45.90633466]]\n",
      "Action:5, Reward:500\n",
      "episode: 278/1000\n",
      "State[CPU, Memory] => [[19.95184217 25.55060626]]\n",
      "Action:5, Reward:500\n",
      "episode: 279/1000\n",
      "State[CPU, Memory] => [[32.4707491  27.70036607]]\n",
      "Action:7, Reward:300\n",
      "episode: 280/1000\n",
      "State[CPU, Memory] => [[15.23174186 66.07915609]]\n",
      "Action:0, Reward:1000\n",
      "episode: 281/1000\n",
      "State[CPU, Memory] => [[92.26753581 58.73122556]]\n",
      "Action:3, Reward:700\n",
      "episode: 282/1000\n",
      "State[CPU, Memory] => [[ 3.6053511  61.02560502]]\n",
      "Action:7, Reward:300\n",
      "episode: 283/1000\n",
      "State[CPU, Memory] => [[35.80631666 93.29102837]]\n",
      "Action:5, Reward:500\n",
      "episode: 284/1000\n",
      "State[CPU, Memory] => [[46.71187197 92.87540641]]\n",
      "Action:2, Reward:800\n",
      "episode: 285/1000\n",
      "State[CPU, Memory] => [[49.01794103 29.31076529]]\n",
      "Action:4, Reward:600\n",
      "episode: 286/1000\n",
      "State[CPU, Memory] => [[22.00200775 22.09745161]]\n",
      "Action:2, Reward:800\n",
      "episode: 287/1000\n",
      "State[CPU, Memory] => [[42.06224826 32.80938611]]\n",
      "Action:6, Reward:400\n",
      "episode: 288/1000\n",
      "State[CPU, Memory] => [[46.81419027 78.91673164]]\n",
      "Action:0, Reward:1000\n",
      "episode: 289/1000\n",
      "State[CPU, Memory] => [[83.7497523  26.42010419]]\n",
      "Action:0, Reward:1000\n",
      "episode: 290/1000\n",
      "State[CPU, Memory] => [[ 7.19971772 71.55808796]]\n",
      "Action:8, Reward:200\n",
      "episode: 291/1000\n",
      "State[CPU, Memory] => [[50.23627993 94.24451636]]\n",
      "Action:6, Reward:400\n",
      "episode: 292/1000\n",
      "State[CPU, Memory] => [[82.81935393 11.96956461]]\n",
      "Action:8, Reward:200\n",
      "episode: 293/1000\n",
      "State[CPU, Memory] => [[5.87213042 7.57478261]]\n",
      "Action:5, Reward:500\n",
      "episode: 294/1000\n",
      "State[CPU, Memory] => [[99.95555858 42.20612841]]\n",
      "Action:5, Reward:500\n",
      "episode: 295/1000\n",
      "State[CPU, Memory] => [[97.10213265 67.02357693]]\n",
      "Action:2, Reward:800\n",
      "episode: 296/1000\n",
      "State[CPU, Memory] => [[94.77360728  6.09117679]]\n",
      "Action:1, Reward:900\n",
      "episode: 297/1000\n",
      "State[CPU, Memory] => [[57.65240388 69.14212359]]\n",
      "Action:4, Reward:600\n",
      "episode: 298/1000\n",
      "State[CPU, Memory] => [[72.25969893 69.14629131]]\n",
      "Action:5, Reward:500\n",
      "episode: 299/1000\n",
      "State[CPU, Memory] => [[42.48033053 70.36161053]]\n",
      "Action:3, Reward:700\n",
      "episode: 300/1000\n",
      "State[CPU, Memory] => [[ 5.4073983  34.13592178]]\n",
      "Action:2, Reward:800\n",
      "episode: 301/1000\n",
      "State[CPU, Memory] => [[87.59119504 14.73169768]]\n",
      "Action:5, Reward:500\n",
      "episode: 302/1000\n",
      "State[CPU, Memory] => [[55.73488556  3.35331148]]\n",
      "Action:7, Reward:300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 303/1000\n",
      "State[CPU, Memory] => [[29.74773721  5.02825547]]\n",
      "Action:5, Reward:500\n",
      "episode: 304/1000\n",
      "State[CPU, Memory] => [[79.60448237 13.6643466 ]]\n",
      "Action:4, Reward:600\n",
      "episode: 305/1000\n",
      "State[CPU, Memory] => [[26.15293591  4.98936451]]\n",
      "Action:1, Reward:900\n",
      "episode: 306/1000\n",
      "State[CPU, Memory] => [[19.29452223 65.92604431]]\n",
      "Action:8, Reward:200\n",
      "episode: 307/1000\n",
      "State[CPU, Memory] => [[11.84554571 74.8423891 ]]\n",
      "Action:7, Reward:300\n",
      "episode: 308/1000\n",
      "State[CPU, Memory] => [[40.10129563 43.07927374]]\n",
      "Action:6, Reward:400\n",
      "episode: 309/1000\n",
      "State[CPU, Memory] => [[48.28722504 81.96162111]]\n",
      "Action:6, Reward:400\n",
      "episode: 310/1000\n",
      "State[CPU, Memory] => [[56.76551366 92.5952803 ]]\n",
      "Action:2, Reward:800\n",
      "episode: 311/1000\n",
      "State[CPU, Memory] => [[ 0.7617319  85.84236511]]\n",
      "Action:0, Reward:1000\n",
      "episode: 312/1000\n",
      "State[CPU, Memory] => [[63.47574206 43.3242725 ]]\n",
      "Action:6, Reward:400\n",
      "episode: 313/1000\n",
      "State[CPU, Memory] => [[85.434654   37.86078578]]\n",
      "Action:2, Reward:800\n",
      "episode: 314/1000\n",
      "State[CPU, Memory] => [[51.86130406 81.83802849]]\n",
      "Action:6, Reward:400\n",
      "episode: 315/1000\n",
      "State[CPU, Memory] => [[44.31046897 19.69443757]]\n",
      "Action:5, Reward:500\n",
      "episode: 316/1000\n",
      "State[CPU, Memory] => [[79.50423011 89.23676475]]\n",
      "Action:3, Reward:700\n",
      "episode: 317/1000\n",
      "State[CPU, Memory] => [[50.18965146 64.78957318]]\n",
      "Action:8, Reward:200\n",
      "episode: 318/1000\n",
      "State[CPU, Memory] => [[26.09330947  1.78185505]]\n",
      "Action:8, Reward:200\n",
      "episode: 319/1000\n",
      "State[CPU, Memory] => [[66.53624577 25.93091246]]\n",
      "Action:7, Reward:300\n",
      "episode: 320/1000\n",
      "State[CPU, Memory] => [[22.71643053 27.68772398]]\n",
      "Action:6, Reward:400\n",
      "episode: 321/1000\n",
      "State[CPU, Memory] => [[20.21304417 78.05885095]]\n",
      "Action:8, Reward:200\n",
      "episode: 322/1000\n",
      "State[CPU, Memory] => [[ 5.05083148 65.92869127]]\n",
      "Action:8, Reward:200\n",
      "episode: 323/1000\n",
      "State[CPU, Memory] => [[70.06899559 20.2192601 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 324/1000\n",
      "State[CPU, Memory] => [[ 0.57597261 77.43335938]]\n",
      "Action:8, Reward:200\n",
      "episode: 325/1000\n",
      "State[CPU, Memory] => [[67.33998639 82.71622873]]\n",
      "Action:2, Reward:800\n",
      "episode: 326/1000\n",
      "State[CPU, Memory] => [[38.29499263 12.91633078]]\n",
      "Action:4, Reward:600\n",
      "episode: 327/1000\n",
      "State[CPU, Memory] => [[65.38599959 25.44880692]]\n",
      "Action:4, Reward:600\n",
      "episode: 328/1000\n",
      "State[CPU, Memory] => [[88.35819659 28.98651453]]\n",
      "Action:1, Reward:900\n",
      "episode: 329/1000\n",
      "State[CPU, Memory] => [[20.8516608  52.12901469]]\n",
      "Action:8, Reward:200\n",
      "episode: 330/1000\n",
      "State[CPU, Memory] => [[67.30573407 77.05114947]]\n",
      "Action:3, Reward:700\n",
      "episode: 331/1000\n",
      "State[CPU, Memory] => [[74.42956374 83.51140111]]\n",
      "Action:7, Reward:300\n",
      "episode: 332/1000\n",
      "State[CPU, Memory] => [[14.23765552 89.13597924]]\n",
      "Action:1, Reward:900\n",
      "episode: 333/1000\n",
      "State[CPU, Memory] => [[35.91220539 43.86947489]]\n",
      "Action:2, Reward:800\n",
      "episode: 334/1000\n",
      "State[CPU, Memory] => [[89.1714994  62.34942965]]\n",
      "Action:0, Reward:1000\n",
      "episode: 335/1000\n",
      "State[CPU, Memory] => [[90.44203536 10.76278198]]\n",
      "Action:0, Reward:1000\n",
      "episode: 336/1000\n",
      "State[CPU, Memory] => [[30.88730525 13.65171963]]\n",
      "Action:6, Reward:400\n",
      "episode: 337/1000\n",
      "State[CPU, Memory] => [[80.94412503 45.87872395]]\n",
      "Action:0, Reward:1000\n",
      "episode: 338/1000\n",
      "State[CPU, Memory] => [[41.33334468 20.66957761]]\n",
      "Action:3, Reward:700\n",
      "episode: 339/1000\n",
      "State[CPU, Memory] => [[87.9768482   3.13110392]]\n",
      "Action:5, Reward:500\n",
      "episode: 340/1000\n",
      "State[CPU, Memory] => [[79.05819985 58.31968185]]\n",
      "Action:2, Reward:800\n",
      "episode: 341/1000\n",
      "State[CPU, Memory] => [[97.29683453 18.92513905]]\n",
      "Action:8, Reward:200\n",
      "episode: 342/1000\n",
      "State[CPU, Memory] => [[76.12084523 95.16015018]]\n",
      "Action:7, Reward:300\n",
      "episode: 343/1000\n",
      "State[CPU, Memory] => [[96.83383986 55.67495645]]\n",
      "Action:7, Reward:300\n",
      "episode: 344/1000\n",
      "State[CPU, Memory] => [[91.73697042 90.8636449 ]]\n",
      "Action:3, Reward:700\n",
      "episode: 345/1000\n",
      "State[CPU, Memory] => [[58.02029122 75.53350963]]\n",
      "Action:4, Reward:600\n",
      "episode: 346/1000\n",
      "State[CPU, Memory] => [[71.30226375 63.0410633 ]]\n",
      "Action:5, Reward:500\n",
      "episode: 347/1000\n",
      "State[CPU, Memory] => [[34.97224536 46.41150307]]\n",
      "Action:2, Reward:800\n",
      "episode: 348/1000\n",
      "State[CPU, Memory] => [[97.17695752  3.71860909]]\n",
      "Action:6, Reward:400\n",
      "episode: 349/1000\n",
      "State[CPU, Memory] => [[11.60277322 78.10400204]]\n",
      "Action:4, Reward:600\n",
      "episode: 350/1000\n",
      "State[CPU, Memory] => [[18.22722304 89.13255486]]\n",
      "Action:3, Reward:700\n",
      "episode: 351/1000\n",
      "State[CPU, Memory] => [[49.65180666  8.07543549]]\n",
      "Action:7, Reward:300\n",
      "episode: 352/1000\n",
      "State[CPU, Memory] => [[41.98998682 44.70768226]]\n",
      "Action:0, Reward:1000\n",
      "episode: 353/1000\n",
      "State[CPU, Memory] => [[11.55011786 35.7551251 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 354/1000\n",
      "State[CPU, Memory] => [[ 1.62573364 44.19121963]]\n",
      "Action:7, Reward:300\n",
      "episode: 355/1000\n",
      "State[CPU, Memory] => [[24.85582487 36.38879929]]\n",
      "Action:3, Reward:700\n",
      "episode: 356/1000\n",
      "State[CPU, Memory] => [[73.4505643  56.52190004]]\n",
      "Action:3, Reward:700\n",
      "episode: 357/1000\n",
      "State[CPU, Memory] => [[95.41085071 79.83150079]]\n",
      "Action:1, Reward:900\n",
      "episode: 358/1000\n",
      "State[CPU, Memory] => [[58.49588092 88.74515992]]\n",
      "Action:1, Reward:900\n",
      "episode: 359/1000\n",
      "State[CPU, Memory] => [[16.64621924 18.23343998]]\n",
      "Action:0, Reward:1000\n",
      "episode: 360/1000\n",
      "State[CPU, Memory] => [[95.85077317 31.29191076]]\n",
      "Action:8, Reward:200\n",
      "episode: 361/1000\n",
      "State[CPU, Memory] => [[73.67278553 49.50600913]]\n",
      "Action:1, Reward:900\n",
      "episode: 362/1000\n",
      "State[CPU, Memory] => [[ 4.08681624 14.66270879]]\n",
      "Action:7, Reward:300\n",
      "episode: 363/1000\n",
      "State[CPU, Memory] => [[35.66502153 67.37618446]]\n",
      "Action:1, Reward:900\n",
      "episode: 364/1000\n",
      "State[CPU, Memory] => [[49.85561489 26.98329263]]\n",
      "Action:2, Reward:800\n",
      "episode: 365/1000\n",
      "State[CPU, Memory] => [[70.57861401 86.15251498]]\n",
      "Action:7, Reward:300\n",
      "episode: 366/1000\n",
      "State[CPU, Memory] => [[37.4670523  90.56805213]]\n",
      "Action:4, Reward:600\n",
      "episode: 367/1000\n",
      "State[CPU, Memory] => [[45.87534549 79.86795483]]\n",
      "Action:3, Reward:700\n",
      "episode: 368/1000\n",
      "State[CPU, Memory] => [[62.75290283 35.18963942]]\n",
      "Action:2, Reward:800\n",
      "episode: 369/1000\n",
      "State[CPU, Memory] => [[89.02285405 75.70494932]]\n",
      "Action:2, Reward:800\n",
      "episode: 370/1000\n",
      "State[CPU, Memory] => [[82.33472865 47.00840343]]\n",
      "Action:1, Reward:900\n",
      "episode: 371/1000\n",
      "State[CPU, Memory] => [[45.71027649 63.23088574]]\n",
      "Action:5, Reward:500\n",
      "episode: 372/1000\n",
      "State[CPU, Memory] => [[39.79476696  8.86492305]]\n",
      "Action:7, Reward:300\n",
      "episode: 373/1000\n",
      "State[CPU, Memory] => [[90.98488577 45.61682452]]\n",
      "Action:5, Reward:500\n",
      "episode: 374/1000\n",
      "State[CPU, Memory] => [[21.43609999  4.16936302]]\n",
      "Action:3, Reward:700\n",
      "episode: 375/1000\n",
      "State[CPU, Memory] => [[6.46588869 5.31926528]]\n",
      "Action:2, Reward:800\n",
      "episode: 376/1000\n",
      "State[CPU, Memory] => [[90.77013219 46.7432291 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 377/1000\n",
      "State[CPU, Memory] => [[41.51532638 23.19241173]]\n",
      "Action:4, Reward:600\n",
      "episode: 378/1000\n",
      "State[CPU, Memory] => [[92.01931305 91.69929531]]\n",
      "Action:1, Reward:900\n",
      "episode: 379/1000\n",
      "State[CPU, Memory] => [[94.72727568 74.97416962]]\n",
      "Action:7, Reward:300\n",
      "episode: 380/1000\n",
      "State[CPU, Memory] => [[25.47850505 67.70119538]]\n",
      "Action:8, Reward:200\n",
      "episode: 381/1000\n",
      "State[CPU, Memory] => [[15.8214739  87.20929362]]\n",
      "Action:2, Reward:800\n",
      "episode: 382/1000\n",
      "State[CPU, Memory] => [[36.32488012 74.74390979]]\n",
      "Action:6, Reward:400\n",
      "episode: 383/1000\n",
      "State[CPU, Memory] => [[1.97639442 3.82070197]]\n",
      "Action:3, Reward:700\n",
      "episode: 384/1000\n",
      "State[CPU, Memory] => [[40.69824221 25.72234137]]\n",
      "Action:7, Reward:300\n",
      "episode: 385/1000\n",
      "State[CPU, Memory] => [[20.43898543 24.23508855]]\n",
      "Action:4, Reward:600\n",
      "episode: 386/1000\n",
      "State[CPU, Memory] => [[72.56782561 83.14396792]]\n",
      "Action:3, Reward:700\n",
      "episode: 387/1000\n",
      "State[CPU, Memory] => [[75.22515682 92.52746801]]\n",
      "Action:4, Reward:600\n",
      "episode: 388/1000\n",
      "State[CPU, Memory] => [[33.52511352  9.88150033]]\n",
      "Action:3, Reward:700\n",
      "episode: 389/1000\n",
      "State[CPU, Memory] => [[98.93997868 77.36033018]]\n",
      "Action:6, Reward:400\n",
      "episode: 390/1000\n",
      "State[CPU, Memory] => [[75.42587759 43.82475298]]\n",
      "Action:6, Reward:400\n",
      "episode: 391/1000\n",
      "State[CPU, Memory] => [[78.81218709 59.33466601]]\n",
      "Action:3, Reward:700\n",
      "episode: 392/1000\n",
      "State[CPU, Memory] => [[34.90344322  9.08773254]]\n",
      "Action:8, Reward:200\n",
      "episode: 393/1000\n",
      "State[CPU, Memory] => [[93.27704868 61.1349693 ]]\n",
      "Action:4, Reward:600\n",
      "episode: 394/1000\n",
      "State[CPU, Memory] => [[59.35787029 25.95043397]]\n",
      "Action:6, Reward:400\n",
      "episode: 395/1000\n",
      "State[CPU, Memory] => [[39.43320918 77.32231726]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:0, Reward:1000\n",
      "episode: 396/1000\n",
      "State[CPU, Memory] => [[93.92183178  5.39240051]]\n",
      "Action:6, Reward:400\n",
      "episode: 397/1000\n",
      "State[CPU, Memory] => [[21.97331507 60.24802151]]\n",
      "Action:6, Reward:400\n",
      "episode: 398/1000\n",
      "State[CPU, Memory] => [[15.46771697 99.15965571]]\n",
      "Action:7, Reward:300\n",
      "episode: 399/1000\n",
      "State[CPU, Memory] => [[62.6708661   5.05400231]]\n",
      "Action:3, Reward:700\n",
      "episode: 400/1000\n",
      "State[CPU, Memory] => [[97.08958615 82.48519056]]\n",
      "Action:5, Reward:500\n",
      "episode: 401/1000\n",
      "State[CPU, Memory] => [[46.99971284 67.24770133]]\n",
      "Action:1, Reward:900\n",
      "episode: 402/1000\n",
      "State[CPU, Memory] => [[95.82643142 52.29705682]]\n",
      "Action:3, Reward:700\n",
      "episode: 403/1000\n",
      "State[CPU, Memory] => [[18.97299748  0.59856541]]\n",
      "Action:6, Reward:400\n",
      "episode: 404/1000\n",
      "State[CPU, Memory] => [[16.14120193 51.34012508]]\n",
      "Action:4, Reward:600\n",
      "episode: 405/1000\n",
      "State[CPU, Memory] => [[86.96405894 95.4690438 ]]\n",
      "Action:8, Reward:200\n",
      "episode: 406/1000\n",
      "State[CPU, Memory] => [[53.60102683 36.38325389]]\n",
      "Action:2, Reward:800\n",
      "episode: 407/1000\n",
      "State[CPU, Memory] => [[64.18492072 18.96433786]]\n",
      "Action:8, Reward:200\n",
      "episode: 408/1000\n",
      "State[CPU, Memory] => [[27.65136291  0.39263638]]\n",
      "Action:7, Reward:300\n",
      "episode: 409/1000\n",
      "State[CPU, Memory] => [[7.5989481  9.37309453]]\n",
      "Action:1, Reward:900\n",
      "episode: 410/1000\n",
      "State[CPU, Memory] => [[52.11973823 17.04530957]]\n",
      "Action:6, Reward:400\n",
      "episode: 411/1000\n",
      "State[CPU, Memory] => [[69.97852975 48.13883605]]\n",
      "Action:1, Reward:900\n",
      "episode: 412/1000\n",
      "State[CPU, Memory] => [[10.60199135 45.07825975]]\n",
      "Action:0, Reward:1000\n",
      "episode: 413/1000\n",
      "State[CPU, Memory] => [[12.89732288 89.89332716]]\n",
      "Action:5, Reward:500\n",
      "episode: 414/1000\n",
      "State[CPU, Memory] => [[71.61519578 32.85669732]]\n",
      "Action:4, Reward:600\n",
      "episode: 415/1000\n",
      "State[CPU, Memory] => [[20.09225906 55.43151676]]\n",
      "Action:6, Reward:400\n",
      "episode: 416/1000\n",
      "State[CPU, Memory] => [[29.76718984 62.47369345]]\n",
      "Action:1, Reward:900\n",
      "episode: 417/1000\n",
      "State[CPU, Memory] => [[96.34302868 22.39136001]]\n",
      "Action:0, Reward:1000\n",
      "episode: 418/1000\n",
      "State[CPU, Memory] => [[48.45640437 85.17143612]]\n",
      "Action:0, Reward:1000\n",
      "episode: 419/1000\n",
      "State[CPU, Memory] => [[44.83953852 39.89831992]]\n",
      "Action:6, Reward:400\n",
      "episode: 420/1000\n",
      "State[CPU, Memory] => [[27.34612419 30.88164204]]\n",
      "Action:4, Reward:600\n",
      "episode: 421/1000\n",
      "State[CPU, Memory] => [[71.21398844 56.80423371]]\n",
      "Action:5, Reward:500\n",
      "episode: 422/1000\n",
      "State[CPU, Memory] => [[96.38316269  1.85965636]]\n",
      "Action:8, Reward:200\n",
      "episode: 423/1000\n",
      "State[CPU, Memory] => [[32.2453341  87.73054493]]\n",
      "Action:1, Reward:900\n",
      "episode: 424/1000\n",
      "State[CPU, Memory] => [[71.16159298 75.03512891]]\n",
      "Action:8, Reward:200\n",
      "episode: 425/1000\n",
      "State[CPU, Memory] => [[34.28956995 54.96601779]]\n",
      "Action:5, Reward:500\n",
      "episode: 426/1000\n",
      "State[CPU, Memory] => [[46.8179112  43.26167794]]\n",
      "Action:2, Reward:800\n",
      "episode: 427/1000\n",
      "State[CPU, Memory] => [[76.65446248 62.34733877]]\n",
      "Action:8, Reward:200\n",
      "episode: 428/1000\n",
      "State[CPU, Memory] => [[75.55964545 58.07365497]]\n",
      "Action:3, Reward:700\n",
      "episode: 429/1000\n",
      "State[CPU, Memory] => [[61.24761442 81.35782332]]\n",
      "Action:5, Reward:500\n",
      "episode: 430/1000\n",
      "State[CPU, Memory] => [[53.0012004  59.13348316]]\n",
      "Action:1, Reward:900\n",
      "episode: 431/1000\n",
      "State[CPU, Memory] => [[ 8.00110258 89.16038829]]\n",
      "Action:5, Reward:500\n",
      "episode: 432/1000\n",
      "State[CPU, Memory] => [[85.16685467 34.52229909]]\n",
      "Action:2, Reward:800\n",
      "episode: 433/1000\n",
      "State[CPU, Memory] => [[47.79333281 15.97843735]]\n",
      "Action:8, Reward:200\n",
      "episode: 434/1000\n",
      "State[CPU, Memory] => [[41.26339044 75.69867385]]\n",
      "Action:1, Reward:900\n",
      "episode: 435/1000\n",
      "State[CPU, Memory] => [[82.20159757 43.92970824]]\n",
      "Action:5, Reward:500\n",
      "episode: 436/1000\n",
      "State[CPU, Memory] => [[22.15552419  9.8539317 ]]\n",
      "Action:6, Reward:400\n",
      "episode: 437/1000\n",
      "State[CPU, Memory] => [[66.6017291  45.13581377]]\n",
      "Action:0, Reward:1000\n",
      "episode: 438/1000\n",
      "State[CPU, Memory] => [[59.16713997 52.98451442]]\n",
      "Action:5, Reward:500\n",
      "episode: 439/1000\n",
      "State[CPU, Memory] => [[54.70129568 42.92712395]]\n",
      "Action:0, Reward:1000\n",
      "episode: 440/1000\n",
      "State[CPU, Memory] => [[12.54539521 14.66689003]]\n",
      "Action:5, Reward:500\n",
      "episode: 441/1000\n",
      "State[CPU, Memory] => [[99.74616724 11.58192008]]\n",
      "Action:1, Reward:900\n",
      "episode: 442/1000\n",
      "State[CPU, Memory] => [[35.14528696 76.35092347]]\n",
      "Action:6, Reward:400\n",
      "episode: 443/1000\n",
      "State[CPU, Memory] => [[60.01467975 71.33690519]]\n",
      "Action:3, Reward:700\n",
      "episode: 444/1000\n",
      "State[CPU, Memory] => [[57.49213708 80.43534866]]\n",
      "Action:3, Reward:700\n",
      "episode: 445/1000\n",
      "State[CPU, Memory] => [[18.57306802 89.38731881]]\n",
      "Action:1, Reward:900\n",
      "episode: 446/1000\n",
      "State[CPU, Memory] => [[22.93149431 34.51088202]]\n",
      "Action:5, Reward:500\n",
      "episode: 447/1000\n",
      "State[CPU, Memory] => [[64.66290422  9.16873036]]\n",
      "Action:6, Reward:400\n",
      "episode: 448/1000\n",
      "State[CPU, Memory] => [[83.95747781 53.57871766]]\n",
      "Action:3, Reward:700\n",
      "episode: 449/1000\n",
      "State[CPU, Memory] => [[48.33150306  8.88632024]]\n",
      "Action:5, Reward:500\n",
      "episode: 450/1000\n",
      "State[CPU, Memory] => [[33.00192518 40.75791379]]\n",
      "Action:4, Reward:600\n",
      "episode: 451/1000\n",
      "State[CPU, Memory] => [[99.08610971 99.04768928]]\n",
      "Action:1, Reward:900\n",
      "episode: 452/1000\n",
      "State[CPU, Memory] => [[39.79704371 28.62609952]]\n",
      "Action:8, Reward:200\n",
      "episode: 453/1000\n",
      "State[CPU, Memory] => [[93.68702072 62.37505508]]\n",
      "Action:8, Reward:200\n",
      "episode: 454/1000\n",
      "State[CPU, Memory] => [[88.03413931 30.53905999]]\n",
      "Action:2, Reward:800\n",
      "episode: 455/1000\n",
      "State[CPU, Memory] => [[9.16323918e+01 3.75847757e-02]]\n",
      "Action:7, Reward:300\n",
      "episode: 456/1000\n",
      "State[CPU, Memory] => [[34.12809656 49.2181745 ]]\n",
      "Action:6, Reward:400\n",
      "episode: 457/1000\n",
      "State[CPU, Memory] => [[47.96017929 67.64040877]]\n",
      "Action:8, Reward:200\n",
      "episode: 458/1000\n",
      "State[CPU, Memory] => [[57.29591487 91.2691939 ]]\n",
      "Action:0, Reward:1000\n",
      "episode: 459/1000\n",
      "State[CPU, Memory] => [[18.54258648  8.96709847]]\n",
      "Action:8, Reward:200\n",
      "episode: 460/1000\n",
      "State[CPU, Memory] => [[91.30599296 52.9928834 ]]\n",
      "Action:6, Reward:400\n",
      "episode: 461/1000\n",
      "State[CPU, Memory] => [[63.10499988 54.60689013]]\n",
      "Action:5, Reward:500\n",
      "episode: 462/1000\n",
      "State[CPU, Memory] => [[87.81506025 40.00652602]]\n",
      "Action:2, Reward:800\n",
      "episode: 463/1000\n",
      "State[CPU, Memory] => [[68.41428827 47.9004632 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 464/1000\n",
      "State[CPU, Memory] => [[36.06312268 50.44730816]]\n",
      "Action:8, Reward:200\n",
      "episode: 465/1000\n",
      "State[CPU, Memory] => [[39.76540869 80.30914106]]\n",
      "Action:8, Reward:200\n",
      "episode: 466/1000\n",
      "State[CPU, Memory] => [[77.29552496 80.82058096]]\n",
      "Action:4, Reward:600\n",
      "episode: 467/1000\n",
      "State[CPU, Memory] => [[40.55651644 57.902875  ]]\n",
      "Action:1, Reward:900\n",
      "episode: 468/1000\n",
      "State[CPU, Memory] => [[95.55495749 18.05634405]]\n",
      "Action:4, Reward:600\n",
      "episode: 469/1000\n",
      "State[CPU, Memory] => [[12.66298309 98.31241419]]\n",
      "Action:2, Reward:800\n",
      "episode: 470/1000\n",
      "State[CPU, Memory] => [[53.07160224 31.79823801]]\n",
      "Action:7, Reward:300\n",
      "episode: 471/1000\n",
      "State[CPU, Memory] => [[69.40462822 83.18024637]]\n",
      "Action:7, Reward:300\n",
      "episode: 472/1000\n",
      "State[CPU, Memory] => [[89.14732362  7.24581258]]\n",
      "Action:2, Reward:800\n",
      "episode: 473/1000\n",
      "State[CPU, Memory] => [[68.40260889 76.60464995]]\n",
      "Action:1, Reward:900\n",
      "episode: 474/1000\n",
      "State[CPU, Memory] => [[44.05992963 48.98433228]]\n",
      "Action:3, Reward:700\n",
      "episode: 475/1000\n",
      "State[CPU, Memory] => [[25.78380227 16.10448121]]\n",
      "Action:4, Reward:600\n",
      "episode: 476/1000\n",
      "State[CPU, Memory] => [[17.81021399 60.14136267]]\n",
      "Action:7, Reward:300\n",
      "episode: 477/1000\n",
      "State[CPU, Memory] => [[17.71668474 30.71074608]]\n",
      "Action:6, Reward:400\n",
      "episode: 478/1000\n",
      "State[CPU, Memory] => [[22.28703127 30.57480223]]\n",
      "Action:1, Reward:900\n",
      "episode: 479/1000\n",
      "State[CPU, Memory] => [[93.34320214 12.78879889]]\n",
      "Action:6, Reward:400\n",
      "episode: 480/1000\n",
      "State[CPU, Memory] => [[56.68340956 30.64155288]]\n",
      "Action:7, Reward:300\n",
      "episode: 481/1000\n",
      "State[CPU, Memory] => [[58.16088767 51.15837568]]\n",
      "Action:6, Reward:400\n",
      "episode: 482/1000\n",
      "State[CPU, Memory] => [[ 4.61845757 73.92891955]]\n",
      "Action:4, Reward:600\n",
      "episode: 483/1000\n",
      "State[CPU, Memory] => [[34.4533154  92.25742901]]\n",
      "Action:6, Reward:400\n",
      "episode: 484/1000\n",
      "State[CPU, Memory] => [[ 5.7904555  58.30804987]]\n",
      "Action:6, Reward:400\n",
      "episode: 485/1000\n",
      "State[CPU, Memory] => [[ 9.10489662 11.67965394]]\n",
      "Action:7, Reward:300\n",
      "episode: 486/1000\n",
      "State[CPU, Memory] => [[15.82206819  7.73331391]]\n",
      "Action:4, Reward:600\n",
      "episode: 487/1000\n",
      "State[CPU, Memory] => [[69.86038465 90.54368592]]\n",
      "Action:1, Reward:900\n",
      "episode: 488/1000\n",
      "State[CPU, Memory] => [[26.74161422 57.86847591]]\n",
      "Action:2, Reward:800\n",
      "episode: 489/1000\n",
      "State[CPU, Memory] => [[43.24586096 99.33860428]]\n",
      "Action:7, Reward:300\n",
      "episode: 490/1000\n",
      "State[CPU, Memory] => [[52.93390195  8.66650127]]\n",
      "Action:8, Reward:200\n",
      "episode: 491/1000\n",
      "State[CPU, Memory] => [[39.69081273  6.35880913]]\n",
      "Action:4, Reward:600\n",
      "episode: 492/1000\n",
      "State[CPU, Memory] => [[ 0.54030025 99.64550641]]\n",
      "Action:1, Reward:900\n",
      "episode: 493/1000\n",
      "State[CPU, Memory] => [[77.93123912 92.97257001]]\n",
      "Action:1, Reward:900\n",
      "episode: 494/1000\n",
      "State[CPU, Memory] => [[60.74675799 56.23375131]]\n",
      "Action:0, Reward:1000\n",
      "episode: 495/1000\n",
      "State[CPU, Memory] => [[0.90535769 4.05497776]]\n",
      "Action:4, Reward:600\n",
      "episode: 496/1000\n",
      "State[CPU, Memory] => [[81.03103557  3.78309822]]\n",
      "Action:0, Reward:1000\n",
      "episode: 497/1000\n",
      "State[CPU, Memory] => [[16.90094875 34.87353362]]\n",
      "Action:6, Reward:400\n",
      "episode: 498/1000\n",
      "State[CPU, Memory] => [[25.84917584 18.45311916]]\n",
      "Action:1, Reward:900\n",
      "episode: 499/1000\n",
      "State[CPU, Memory] => [[28.86960785 75.08064853]]\n",
      "Action:1, Reward:900\n",
      "episode: 500/1000\n",
      "State[CPU, Memory] => [[41.31558338 96.48219723]]\n",
      "Action:7, Reward:300\n",
      "episode: 501/1000\n",
      "State[CPU, Memory] => [[55.0022     74.63030658]]\n",
      "Action:0, Reward:1000\n",
      "episode: 502/1000\n",
      "State[CPU, Memory] => [[ 1.67733491 37.47146104]]\n",
      "Action:8, Reward:200\n",
      "episode: 503/1000\n",
      "State[CPU, Memory] => [[49.03656193 72.69786883]]\n",
      "Action:2, Reward:800\n",
      "episode: 504/1000\n",
      "State[CPU, Memory] => [[98.63710758 11.33898041]]\n",
      "Action:1, Reward:900\n",
      "episode: 505/1000\n",
      "State[CPU, Memory] => [[ 5.16111691 62.28085712]]\n",
      "Action:6, Reward:400\n",
      "episode: 506/1000\n",
      "State[CPU, Memory] => [[46.08721081  7.82030627]]\n",
      "Action:0, Reward:1000\n",
      "episode: 507/1000\n",
      "State[CPU, Memory] => [[27.54021456 18.50822687]]\n",
      "Action:6, Reward:400\n",
      "episode: 508/1000\n",
      "State[CPU, Memory] => [[60.61978602 70.88726321]]\n",
      "Action:4, Reward:600\n",
      "episode: 509/1000\n",
      "State[CPU, Memory] => [[61.92928985 93.75232976]]\n",
      "Action:2, Reward:800\n",
      "episode: 510/1000\n",
      "State[CPU, Memory] => [[14.18397513  4.3377007 ]]\n",
      "Action:6, Reward:400\n",
      "episode: 511/1000\n",
      "State[CPU, Memory] => [[19.50265855  9.93987982]]\n",
      "Action:8, Reward:200\n",
      "episode: 512/1000\n",
      "State[CPU, Memory] => [[66.56493796 63.48895307]]\n",
      "Action:2, Reward:800\n",
      "episode: 513/1000\n",
      "State[CPU, Memory] => [[62.71859064 73.98250219]]\n",
      "Action:2, Reward:800\n",
      "episode: 514/1000\n",
      "State[CPU, Memory] => [[56.18708604 62.31229605]]\n",
      "Action:5, Reward:500\n",
      "episode: 515/1000\n",
      "State[CPU, Memory] => [[ 0.6354225  24.49394176]]\n",
      "Action:3, Reward:700\n",
      "episode: 516/1000\n",
      "State[CPU, Memory] => [[35.4979374 79.9671524]]\n",
      "Action:1, Reward:900\n",
      "episode: 517/1000\n",
      "State[CPU, Memory] => [[82.51964775 91.03677047]]\n",
      "Action:1, Reward:900\n",
      "episode: 518/1000\n",
      "State[CPU, Memory] => [[60.50448126 96.24873014]]\n",
      "Action:5, Reward:500\n",
      "episode: 519/1000\n",
      "State[CPU, Memory] => [[23.65388589 19.62049105]]\n",
      "Action:1, Reward:900\n",
      "episode: 520/1000\n",
      "State[CPU, Memory] => [[41.05646923 75.71438689]]\n",
      "Action:3, Reward:700\n",
      "episode: 521/1000\n",
      "State[CPU, Memory] => [[74.60715888 24.60707962]]\n",
      "Action:0, Reward:1000\n",
      "episode: 522/1000\n",
      "State[CPU, Memory] => [[66.71108369 17.70072344]]\n",
      "Action:7, Reward:300\n",
      "episode: 523/1000\n",
      "State[CPU, Memory] => [[15.13239791 71.77081839]]\n",
      "Action:4, Reward:600\n",
      "episode: 524/1000\n",
      "State[CPU, Memory] => [[30.70079737 63.35491458]]\n",
      "Action:2, Reward:800\n",
      "episode: 525/1000\n",
      "State[CPU, Memory] => [[18.09139875 93.11887357]]\n",
      "Action:5, Reward:500\n",
      "episode: 526/1000\n",
      "State[CPU, Memory] => [[72.93888114 34.45652862]]\n",
      "Action:6, Reward:400\n",
      "episode: 527/1000\n",
      "State[CPU, Memory] => [[41.94401184 34.43809125]]\n",
      "Action:3, Reward:700\n",
      "episode: 528/1000\n",
      "State[CPU, Memory] => [[ 8.54873914 51.60543305]]\n",
      "Action:2, Reward:800\n",
      "episode: 529/1000\n",
      "State[CPU, Memory] => [[80.73118954 86.42218384]]\n",
      "Action:3, Reward:700\n",
      "episode: 530/1000\n",
      "State[CPU, Memory] => [[84.06797493 11.29385819]]\n",
      "Action:3, Reward:700\n",
      "episode: 531/1000\n",
      "State[CPU, Memory] => [[92.18761922 58.09501982]]\n",
      "Action:6, Reward:400\n",
      "episode: 532/1000\n",
      "State[CPU, Memory] => [[90.95170159 99.41988585]]\n",
      "Action:5, Reward:500\n",
      "episode: 533/1000\n",
      "State[CPU, Memory] => [[59.7087055  16.57127738]]\n",
      "Action:0, Reward:1000\n",
      "episode: 534/1000\n",
      "State[CPU, Memory] => [[72.35855939 78.29230152]]\n",
      "Action:6, Reward:400\n",
      "episode: 535/1000\n",
      "State[CPU, Memory] => [[48.16293677 95.73131167]]\n",
      "Action:3, Reward:700\n",
      "episode: 536/1000\n",
      "State[CPU, Memory] => [[69.2656698  31.55183331]]\n",
      "Action:4, Reward:600\n",
      "episode: 537/1000\n",
      "State[CPU, Memory] => [[74.27900477 65.92145649]]\n",
      "Action:2, Reward:800\n",
      "episode: 538/1000\n",
      "State[CPU, Memory] => [[23.8948121   7.19177411]]\n",
      "Action:4, Reward:600\n",
      "episode: 539/1000\n",
      "State[CPU, Memory] => [[90.48043457 88.82685309]]\n",
      "Action:4, Reward:600\n",
      "episode: 540/1000\n",
      "State[CPU, Memory] => [[44.73329156 11.13016822]]\n",
      "Action:2, Reward:800\n",
      "episode: 541/1000\n",
      "State[CPU, Memory] => [[31.73757319 77.38690283]]\n",
      "Action:1, Reward:900\n",
      "episode: 542/1000\n",
      "State[CPU, Memory] => [[27.73661025 48.22214628]]\n",
      "Action:4, Reward:600\n",
      "episode: 543/1000\n",
      "State[CPU, Memory] => [[ 1.91315853 55.60436879]]\n",
      "Action:4, Reward:600\n",
      "episode: 544/1000\n",
      "State[CPU, Memory] => [[35.98256243  0.51047213]]\n",
      "Action:1, Reward:900\n",
      "episode: 545/1000\n",
      "State[CPU, Memory] => [[ 9.22375055 70.72243523]]\n",
      "Action:5, Reward:500\n",
      "episode: 546/1000\n",
      "State[CPU, Memory] => [[15.22319053 42.93154963]]\n",
      "Action:7, Reward:300\n",
      "episode: 547/1000\n",
      "State[CPU, Memory] => [[97.36526562 75.21474491]]\n",
      "Action:3, Reward:700\n",
      "episode: 548/1000\n",
      "State[CPU, Memory] => [[ 4.91319069 12.52680617]]\n",
      "Action:1, Reward:900\n",
      "episode: 549/1000\n",
      "State[CPU, Memory] => [[85.12086682 76.97769441]]\n",
      "Action:6, Reward:400\n",
      "episode: 550/1000\n",
      "State[CPU, Memory] => [[59.90551327  0.73572477]]\n",
      "Action:8, Reward:200\n",
      "episode: 551/1000\n",
      "State[CPU, Memory] => [[51.77736654 75.26136902]]\n",
      "Action:7, Reward:300\n",
      "episode: 552/1000\n",
      "State[CPU, Memory] => [[30.80398589 21.55630472]]\n",
      "Action:8, Reward:200\n",
      "episode: 553/1000\n",
      "State[CPU, Memory] => [[36.41416673 42.33642395]]\n",
      "Action:7, Reward:300\n",
      "episode: 554/1000\n",
      "State[CPU, Memory] => [[17.33029226 77.12464722]]\n",
      "Action:3, Reward:700\n",
      "episode: 555/1000\n",
      "State[CPU, Memory] => [[45.56360539 29.9020123 ]]\n",
      "Action:5, Reward:500\n",
      "episode: 556/1000\n",
      "State[CPU, Memory] => [[ 7.72405693 73.41295547]]\n",
      "Action:4, Reward:600\n",
      "episode: 557/1000\n",
      "State[CPU, Memory] => [[24.36654202 80.04327436]]\n",
      "Action:8, Reward:200\n",
      "episode: 558/1000\n",
      "State[CPU, Memory] => [[64.49038561 21.33381444]]\n",
      "Action:2, Reward:800\n",
      "episode: 559/1000\n",
      "State[CPU, Memory] => [[90.52058055 45.53562111]]\n",
      "Action:7, Reward:300\n",
      "episode: 560/1000\n",
      "State[CPU, Memory] => [[9.31238485 7.63208306]]\n",
      "Action:3, Reward:700\n",
      "episode: 561/1000\n",
      "State[CPU, Memory] => [[69.63062666 92.09339865]]\n",
      "Action:0, Reward:1000\n",
      "episode: 562/1000\n",
      "State[CPU, Memory] => [[26.24812011  4.046718  ]]\n",
      "Action:4, Reward:600\n",
      "episode: 563/1000\n",
      "State[CPU, Memory] => [[20.68342187 39.41992736]]\n",
      "Action:1, Reward:900\n",
      "episode: 564/1000\n",
      "State[CPU, Memory] => [[24.64618449 57.60889658]]\n",
      "Action:6, Reward:400\n",
      "episode: 565/1000\n",
      "State[CPU, Memory] => [[24.70297648 48.397089  ]]\n",
      "Action:0, Reward:1000\n",
      "episode: 566/1000\n",
      "State[CPU, Memory] => [[ 4.82405135 92.89309083]]\n",
      "Action:3, Reward:700\n",
      "episode: 567/1000\n",
      "State[CPU, Memory] => [[11.14378261 67.33432432]]\n",
      "Action:6, Reward:400\n",
      "episode: 568/1000\n",
      "State[CPU, Memory] => [[66.68889553 45.68335258]]\n",
      "Action:7, Reward:300\n",
      "episode: 569/1000\n",
      "State[CPU, Memory] => [[50.8295724 11.1058786]]\n",
      "Action:1, Reward:900\n",
      "episode: 570/1000\n",
      "State[CPU, Memory] => [[91.53738788  4.28569514]]\n",
      "Action:3, Reward:700\n",
      "episode: 571/1000\n",
      "State[CPU, Memory] => [[59.58332549 66.00669251]]\n",
      "Action:0, Reward:1000\n",
      "episode: 572/1000\n",
      "State[CPU, Memory] => [[60.44749296 41.12836516]]\n",
      "Action:4, Reward:600\n",
      "episode: 573/1000\n",
      "State[CPU, Memory] => [[63.10610289 60.36148284]]\n",
      "Action:6, Reward:400\n",
      "episode: 574/1000\n",
      "State[CPU, Memory] => [[72.08351381 90.10364011]]\n",
      "Action:2, Reward:800\n",
      "episode: 575/1000\n",
      "State[CPU, Memory] => [[32.9128208  90.32008638]]\n",
      "Action:2, Reward:800\n",
      "episode: 576/1000\n",
      "State[CPU, Memory] => [[62.74096941 52.30792435]]\n",
      "Action:7, Reward:300\n",
      "episode: 577/1000\n",
      "State[CPU, Memory] => [[62.38592095  5.28843226]]\n",
      "Action:4, Reward:600\n",
      "episode: 578/1000\n",
      "State[CPU, Memory] => [[74.59305363 44.82071747]]\n",
      "Action:1, Reward:900\n",
      "episode: 579/1000\n",
      "State[CPU, Memory] => [[97.89617635 41.17931565]]\n",
      "Action:5, Reward:500\n",
      "episode: 580/1000\n",
      "State[CPU, Memory] => [[92.00149297 83.58939117]]\n",
      "Action:7, Reward:300\n",
      "episode: 581/1000\n",
      "State[CPU, Memory] => [[81.47283086 62.2317439 ]]\n",
      "Action:8, Reward:200\n",
      "episode: 582/1000\n",
      "State[CPU, Memory] => [[ 5.20719706 89.25019209]]\n",
      "Action:2, Reward:800\n",
      "episode: 583/1000\n",
      "State[CPU, Memory] => [[88.18200172 17.91538751]]\n",
      "Action:0, Reward:1000\n",
      "episode: 584/1000\n",
      "State[CPU, Memory] => [[98.56656575  9.67230463]]\n",
      "Action:3, Reward:700\n",
      "episode: 585/1000\n",
      "State[CPU, Memory] => [[31.35919889 85.98161905]]\n",
      "Action:3, Reward:700\n",
      "episode: 586/1000\n",
      "State[CPU, Memory] => [[97.10559638 66.78902833]]\n",
      "Action:8, Reward:200\n",
      "episode: 587/1000\n",
      "State[CPU, Memory] => [[ 4.20191343 41.48523181]]\n",
      "Action:7, Reward:300\n",
      "episode: 588/1000\n",
      "State[CPU, Memory] => [[63.56833113 45.56881016]]\n",
      "Action:7, Reward:300\n",
      "episode: 589/1000\n",
      "State[CPU, Memory] => [[ 1.49637579 13.90182638]]\n",
      "Action:7, Reward:300\n",
      "episode: 590/1000\n",
      "State[CPU, Memory] => [[83.01585224 34.23500257]]\n",
      "Action:4, Reward:600\n",
      "episode: 591/1000\n",
      "State[CPU, Memory] => [[94.40810285 67.51426213]]\n",
      "Action:8, Reward:200\n",
      "episode: 592/1000\n",
      "State[CPU, Memory] => [[53.07888746 39.63396048]]\n",
      "Action:4, Reward:600\n",
      "episode: 593/1000\n",
      "State[CPU, Memory] => [[13.12235676 72.61688012]]\n",
      "Action:0, Reward:1000\n",
      "episode: 594/1000\n",
      "State[CPU, Memory] => [[71.63771823 43.85433869]]\n",
      "Action:7, Reward:300\n",
      "episode: 595/1000\n",
      "State[CPU, Memory] => [[ 8.40920202 39.58146341]]\n",
      "Action:0, Reward:1000\n",
      "episode: 596/1000\n",
      "State[CPU, Memory] => [[63.33839511 63.17588567]]\n",
      "Action:6, Reward:400\n",
      "episode: 597/1000\n",
      "State[CPU, Memory] => [[8.63252608 4.36913939]]\n",
      "Action:0, Reward:1000\n",
      "episode: 598/1000\n",
      "State[CPU, Memory] => [[42.50425129 47.77000096]]\n",
      "Action:4, Reward:600\n",
      "episode: 599/1000\n",
      "State[CPU, Memory] => [[57.60442495 83.11706252]]\n",
      "Action:7, Reward:300\n",
      "episode: 600/1000\n",
      "State[CPU, Memory] => [[54.38543718 99.23930781]]\n",
      "Action:5, Reward:500\n",
      "episode: 601/1000\n",
      "State[CPU, Memory] => [[77.70123607 24.66459818]]\n",
      "Action:2, Reward:800\n",
      "episode: 602/1000\n",
      "State[CPU, Memory] => [[67.00167662 61.37010933]]\n",
      "Action:1, Reward:900\n",
      "episode: 603/1000\n",
      "State[CPU, Memory] => [[18.54310738 23.32557383]]\n",
      "Action:6, Reward:400\n",
      "episode: 604/1000\n",
      "State[CPU, Memory] => [[49.29227305 62.12191509]]\n",
      "Action:4, Reward:600\n",
      "episode: 605/1000\n",
      "State[CPU, Memory] => [[55.00944651 65.87643977]]\n",
      "Action:7, Reward:300\n",
      "episode: 606/1000\n",
      "State[CPU, Memory] => [[51.80202444 58.91219849]]\n",
      "Action:0, Reward:1000\n",
      "episode: 607/1000\n",
      "State[CPU, Memory] => [[93.0163746  37.99154511]]\n",
      "Action:5, Reward:500\n",
      "episode: 608/1000\n",
      "State[CPU, Memory] => [[ 8.50264612 27.37368098]]\n",
      "Action:3, Reward:700\n",
      "episode: 609/1000\n",
      "State[CPU, Memory] => [[61.18009893 88.7572778 ]]\n",
      "Action:6, Reward:400\n",
      "episode: 610/1000\n",
      "State[CPU, Memory] => [[38.63941096  5.85060138]]\n",
      "Action:5, Reward:500\n",
      "episode: 611/1000\n",
      "State[CPU, Memory] => [[17.99765265 30.16810439]]\n",
      "Action:5, Reward:500\n",
      "episode: 612/1000\n",
      "State[CPU, Memory] => [[35.23098875 99.55895265]]\n",
      "Action:3, Reward:700\n",
      "episode: 613/1000\n",
      "State[CPU, Memory] => [[78.49641811 73.94736826]]\n",
      "Action:3, Reward:700\n",
      "episode: 614/1000\n",
      "State[CPU, Memory] => [[19.617141    5.68049559]]\n",
      "Action:4, Reward:600\n",
      "episode: 615/1000\n",
      "State[CPU, Memory] => [[13.40639798 94.04809379]]\n",
      "Action:3, Reward:700\n",
      "episode: 616/1000\n",
      "State[CPU, Memory] => [[ 9.05501549 36.14983177]]\n",
      "Action:2, Reward:800\n",
      "episode: 617/1000\n",
      "State[CPU, Memory] => [[73.2727018  31.95470556]]\n",
      "Action:0, Reward:1000\n",
      "episode: 618/1000\n",
      "State[CPU, Memory] => [[40.253231   41.74108551]]\n",
      "Action:0, Reward:1000\n",
      "episode: 619/1000\n",
      "State[CPU, Memory] => [[62.41725358 14.01710808]]\n",
      "Action:8, Reward:200\n",
      "episode: 620/1000\n",
      "State[CPU, Memory] => [[32.59401327  4.84080831]]\n",
      "Action:8, Reward:200\n",
      "episode: 621/1000\n",
      "State[CPU, Memory] => [[85.40213309 53.44190939]]\n",
      "Action:1, Reward:900\n",
      "episode: 622/1000\n",
      "State[CPU, Memory] => [[10.67826908 41.62220516]]\n",
      "Action:3, Reward:700\n",
      "episode: 623/1000\n",
      "State[CPU, Memory] => [[ 7.58491403 56.872042  ]]\n",
      "Action:8, Reward:200\n",
      "episode: 624/1000\n",
      "State[CPU, Memory] => [[ 7.54991372 52.46071851]]\n",
      "Action:5, Reward:500\n",
      "episode: 625/1000\n",
      "State[CPU, Memory] => [[88.07081759 78.00651636]]\n",
      "Action:1, Reward:900\n",
      "episode: 626/1000\n",
      "State[CPU, Memory] => [[82.97818463 64.24453502]]\n",
      "Action:2, Reward:800\n",
      "episode: 627/1000\n",
      "State[CPU, Memory] => [[ 1.72614386 43.6990506 ]]\n",
      "Action:2, Reward:800\n",
      "episode: 628/1000\n",
      "State[CPU, Memory] => [[91.88912901 43.82018781]]\n",
      "Action:8, Reward:200\n",
      "episode: 629/1000\n",
      "State[CPU, Memory] => [[ 4.10966216 13.49344121]]\n",
      "Action:1, Reward:900\n",
      "episode: 630/1000\n",
      "State[CPU, Memory] => [[75.11584821 64.98267072]]\n",
      "Action:2, Reward:800\n",
      "episode: 631/1000\n",
      "State[CPU, Memory] => [[91.07453751 78.98589135]]\n",
      "Action:3, Reward:700\n",
      "episode: 632/1000\n",
      "State[CPU, Memory] => [[79.77575993  1.61420288]]\n",
      "Action:8, Reward:200\n",
      "episode: 633/1000\n",
      "State[CPU, Memory] => [[91.40988148 69.11274017]]\n",
      "Action:4, Reward:600\n",
      "episode: 634/1000\n",
      "State[CPU, Memory] => [[23.3827333  36.78725758]]\n",
      "Action:5, Reward:500\n",
      "episode: 635/1000\n",
      "State[CPU, Memory] => [[26.74155143 64.15649749]]\n",
      "Action:4, Reward:600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 636/1000\n",
      "State[CPU, Memory] => [[20.46308811  4.96379661]]\n",
      "Action:5, Reward:500\n",
      "episode: 637/1000\n",
      "State[CPU, Memory] => [[38.56984156 59.39591238]]\n",
      "Action:6, Reward:400\n",
      "episode: 638/1000\n",
      "State[CPU, Memory] => [[16.79614626 92.45073708]]\n",
      "Action:8, Reward:200\n",
      "episode: 639/1000\n",
      "State[CPU, Memory] => [[81.28045233 84.92422014]]\n",
      "Action:6, Reward:400\n",
      "episode: 640/1000\n",
      "State[CPU, Memory] => [[66.52867836 85.67895759]]\n",
      "Action:0, Reward:1000\n",
      "episode: 641/1000\n",
      "State[CPU, Memory] => [[57.80360655 11.48720336]]\n",
      "Action:0, Reward:1000\n",
      "episode: 642/1000\n",
      "State[CPU, Memory] => [[68.65691135 10.52378188]]\n",
      "Action:1, Reward:900\n",
      "episode: 643/1000\n",
      "State[CPU, Memory] => [[89.41939097 93.05187814]]\n",
      "Action:5, Reward:500\n",
      "episode: 644/1000\n",
      "State[CPU, Memory] => [[79.0923829   8.28739598]]\n",
      "Action:6, Reward:400\n",
      "episode: 645/1000\n",
      "State[CPU, Memory] => [[46.30551236 34.67288686]]\n",
      "Action:6, Reward:400\n",
      "episode: 646/1000\n",
      "State[CPU, Memory] => [[32.7139375  58.29769817]]\n",
      "Action:2, Reward:800\n",
      "episode: 647/1000\n",
      "State[CPU, Memory] => [[22.71718999 91.11046455]]\n",
      "Action:3, Reward:700\n",
      "episode: 648/1000\n",
      "State[CPU, Memory] => [[84.71567792 33.09731475]]\n",
      "Action:8, Reward:200\n",
      "episode: 649/1000\n",
      "State[CPU, Memory] => [[2.02473096e-03 4.89548771e+01]]\n",
      "Action:1, Reward:900\n",
      "episode: 650/1000\n",
      "State[CPU, Memory] => [[40.87524847 90.67158075]]\n",
      "Action:4, Reward:600\n",
      "episode: 651/1000\n",
      "State[CPU, Memory] => [[95.82944838 40.7844923 ]]\n",
      "Action:8, Reward:200\n",
      "episode: 652/1000\n",
      "State[CPU, Memory] => [[61.21492231 47.96056646]]\n",
      "Action:6, Reward:400\n",
      "episode: 653/1000\n",
      "State[CPU, Memory] => [[50.37518692 94.63831662]]\n",
      "Action:4, Reward:600\n",
      "episode: 654/1000\n",
      "State[CPU, Memory] => [[60.01428089 29.72451264]]\n",
      "Action:8, Reward:200\n",
      "episode: 655/1000\n",
      "State[CPU, Memory] => [[ 5.62169111 40.26813463]]\n",
      "Action:4, Reward:600\n",
      "episode: 656/1000\n",
      "State[CPU, Memory] => [[ 8.14074981 47.29167799]]\n",
      "Action:5, Reward:500\n",
      "episode: 657/1000\n",
      "State[CPU, Memory] => [[87.74508548  1.64741304]]\n",
      "Action:6, Reward:400\n",
      "episode: 658/1000\n",
      "State[CPU, Memory] => [[10.50523209  1.94202439]]\n",
      "Action:0, Reward:1000\n",
      "episode: 659/1000\n",
      "State[CPU, Memory] => [[ 6.7868298  95.65530773]]\n",
      "Action:2, Reward:800\n",
      "episode: 660/1000\n",
      "State[CPU, Memory] => [[19.02508326 44.61443478]]\n",
      "Action:7, Reward:300\n",
      "episode: 661/1000\n",
      "State[CPU, Memory] => [[52.65915668 59.76617086]]\n",
      "Action:7, Reward:300\n",
      "episode: 662/1000\n",
      "State[CPU, Memory] => [[80.3001817   1.67897499]]\n",
      "Action:6, Reward:400\n",
      "episode: 663/1000\n",
      "State[CPU, Memory] => [[18.88987722 40.45431322]]\n",
      "Action:4, Reward:600\n",
      "episode: 664/1000\n",
      "State[CPU, Memory] => [[54.6053332  24.39913465]]\n",
      "Action:4, Reward:600\n",
      "episode: 665/1000\n",
      "State[CPU, Memory] => [[63.34657768 91.00843332]]\n",
      "Action:8, Reward:200\n",
      "episode: 666/1000\n",
      "State[CPU, Memory] => [[41.72131253 50.24334608]]\n",
      "Action:7, Reward:300\n",
      "episode: 667/1000\n",
      "State[CPU, Memory] => [[69.07584746 74.58001936]]\n",
      "Action:7, Reward:300\n",
      "episode: 668/1000\n",
      "State[CPU, Memory] => [[10.01443573 26.21819136]]\n",
      "Action:5, Reward:500\n",
      "episode: 669/1000\n",
      "State[CPU, Memory] => [[57.02877446 85.91930596]]\n",
      "Action:5, Reward:500\n",
      "episode: 670/1000\n",
      "State[CPU, Memory] => [[14.01643384 43.63421622]]\n",
      "Action:1, Reward:900\n",
      "episode: 671/1000\n",
      "State[CPU, Memory] => [[78.89500987 86.75707791]]\n",
      "Action:2, Reward:800\n",
      "episode: 672/1000\n",
      "State[CPU, Memory] => [[ 5.66991953 18.04488492]]\n",
      "Action:4, Reward:600\n",
      "episode: 673/1000\n",
      "State[CPU, Memory] => [[86.85923867 57.93960335]]\n",
      "Action:7, Reward:300\n",
      "episode: 674/1000\n",
      "State[CPU, Memory] => [[14.69663275 23.6305546 ]]\n",
      "Action:4, Reward:600\n",
      "episode: 675/1000\n",
      "State[CPU, Memory] => [[70.6767015   4.17590022]]\n",
      "Action:1, Reward:900\n",
      "episode: 676/1000\n",
      "State[CPU, Memory] => [[58.9039932  91.44056783]]\n",
      "Action:7, Reward:300\n",
      "episode: 677/1000\n",
      "State[CPU, Memory] => [[5.66385420e-02 7.12050874e+01]]\n",
      "Action:8, Reward:200\n",
      "episode: 678/1000\n",
      "State[CPU, Memory] => [[62.57267121 68.87987597]]\n",
      "Action:0, Reward:1000\n",
      "episode: 679/1000\n",
      "State[CPU, Memory] => [[90.2328447  61.88192991]]\n",
      "Action:5, Reward:500\n",
      "episode: 680/1000\n",
      "State[CPU, Memory] => [[40.61122941 22.21402351]]\n",
      "Action:2, Reward:800\n",
      "episode: 681/1000\n",
      "State[CPU, Memory] => [[40.42385867 43.82159124]]\n",
      "Action:7, Reward:300\n",
      "episode: 682/1000\n",
      "State[CPU, Memory] => [[20.70596811 48.62131897]]\n",
      "Action:8, Reward:200\n",
      "episode: 683/1000\n",
      "State[CPU, Memory] => [[97.93099144 55.03590182]]\n",
      "Action:4, Reward:600\n",
      "episode: 684/1000\n",
      "State[CPU, Memory] => [[60.69055281 65.43990871]]\n",
      "Action:8, Reward:200\n",
      "episode: 685/1000\n",
      "State[CPU, Memory] => [[ 4.74522301 50.32273547]]\n",
      "Action:6, Reward:400\n",
      "episode: 686/1000\n",
      "State[CPU, Memory] => [[64.00420367  8.35397811]]\n",
      "Action:3, Reward:700\n",
      "episode: 687/1000\n",
      "State[CPU, Memory] => [[46.82117639 35.03959557]]\n",
      "Action:6, Reward:400\n",
      "episode: 688/1000\n",
      "State[CPU, Memory] => [[68.3684252  91.13063468]]\n",
      "Action:7, Reward:300\n",
      "episode: 689/1000\n",
      "State[CPU, Memory] => [[62.85814996  0.84372717]]\n",
      "Action:5, Reward:500\n",
      "episode: 690/1000\n",
      "State[CPU, Memory] => [[86.44720756 88.80143578]]\n",
      "Action:7, Reward:300\n",
      "episode: 691/1000\n",
      "State[CPU, Memory] => [[11.1252593  72.79683207]]\n",
      "Action:2, Reward:800\n",
      "episode: 692/1000\n",
      "State[CPU, Memory] => [[83.4212528  64.93194519]]\n",
      "Action:3, Reward:700\n",
      "episode: 693/1000\n",
      "State[CPU, Memory] => [[71.47633746 37.64245874]]\n",
      "Action:4, Reward:600\n",
      "episode: 694/1000\n",
      "State[CPU, Memory] => [[97.10763734 50.95446425]]\n",
      "Action:8, Reward:200\n",
      "episode: 695/1000\n",
      "State[CPU, Memory] => [[74.98028408 41.76983498]]\n",
      "Action:3, Reward:700\n",
      "episode: 696/1000\n",
      "State[CPU, Memory] => [[36.49504577 45.82021715]]\n",
      "Action:7, Reward:300\n",
      "episode: 697/1000\n",
      "State[CPU, Memory] => [[39.13644615 11.29865902]]\n",
      "Action:8, Reward:200\n",
      "episode: 698/1000\n",
      "State[CPU, Memory] => [[92.83697713  5.11879705]]\n",
      "Action:6, Reward:400\n",
      "episode: 699/1000\n",
      "State[CPU, Memory] => [[67.24211199 20.76758086]]\n",
      "Action:8, Reward:200\n",
      "episode: 700/1000\n",
      "State[CPU, Memory] => [[15.20200161 56.64466971]]\n",
      "Action:2, Reward:800\n",
      "episode: 701/1000\n",
      "State[CPU, Memory] => [[55.84146369 93.17079707]]\n",
      "Action:1, Reward:900\n",
      "episode: 702/1000\n",
      "State[CPU, Memory] => [[20.4387939  14.09398954]]\n",
      "Action:1, Reward:900\n",
      "episode: 703/1000\n",
      "State[CPU, Memory] => [[22.02962176 28.49642721]]\n",
      "Action:5, Reward:500\n",
      "episode: 704/1000\n",
      "State[CPU, Memory] => [[10.73226432 57.26031475]]\n",
      "Action:3, Reward:700\n",
      "episode: 705/1000\n",
      "State[CPU, Memory] => [[ 7.51967884 79.19536495]]\n",
      "Action:6, Reward:400\n",
      "episode: 706/1000\n",
      "State[CPU, Memory] => [[63.1821523  4.4268448]]\n",
      "Action:2, Reward:800\n",
      "episode: 707/1000\n",
      "State[CPU, Memory] => [[28.86686446 63.71172692]]\n",
      "Action:5, Reward:500\n",
      "episode: 708/1000\n",
      "State[CPU, Memory] => [[ 1.23443922 23.01868064]]\n",
      "Action:8, Reward:200\n",
      "episode: 709/1000\n",
      "State[CPU, Memory] => [[49.62506791  8.18389471]]\n",
      "Action:6, Reward:400\n",
      "episode: 710/1000\n",
      "State[CPU, Memory] => [[ 4.71248566 18.59022097]]\n",
      "Action:0, Reward:1000\n",
      "episode: 711/1000\n",
      "State[CPU, Memory] => [[67.10609173 81.35077691]]\n",
      "Action:4, Reward:600\n",
      "episode: 712/1000\n",
      "State[CPU, Memory] => [[5.62173491 1.5242494 ]]\n",
      "Action:3, Reward:700\n",
      "episode: 713/1000\n",
      "State[CPU, Memory] => [[17.09280719 82.25886885]]\n",
      "Action:8, Reward:200\n",
      "episode: 714/1000\n",
      "State[CPU, Memory] => [[96.4697471   4.60495388]]\n",
      "Action:2, Reward:800\n",
      "episode: 715/1000\n",
      "State[CPU, Memory] => [[34.867362   91.67577105]]\n",
      "Action:8, Reward:200\n",
      "episode: 716/1000\n",
      "State[CPU, Memory] => [[87.28846883 98.42065908]]\n",
      "Action:0, Reward:1000\n",
      "episode: 717/1000\n",
      "State[CPU, Memory] => [[80.286011   45.80467465]]\n",
      "Action:2, Reward:800\n",
      "episode: 718/1000\n",
      "State[CPU, Memory] => [[21.63013336 83.81683832]]\n",
      "Action:5, Reward:500\n",
      "episode: 719/1000\n",
      "State[CPU, Memory] => [[71.24560213 45.15406219]]\n",
      "Action:1, Reward:900\n",
      "episode: 720/1000\n",
      "State[CPU, Memory] => [[72.56619357 54.14906402]]\n",
      "Action:7, Reward:300\n",
      "episode: 721/1000\n",
      "State[CPU, Memory] => [[80.1127354   9.75005739]]\n",
      "Action:5, Reward:500\n",
      "episode: 722/1000\n",
      "State[CPU, Memory] => [[10.29539535 21.56197613]]\n",
      "Action:0, Reward:1000\n",
      "episode: 723/1000\n",
      "State[CPU, Memory] => [[ 0.69749168 14.74793178]]\n",
      "Action:1, Reward:900\n",
      "episode: 724/1000\n",
      "State[CPU, Memory] => [[90.02034395 50.72182757]]\n",
      "Action:4, Reward:600\n",
      "episode: 725/1000\n",
      "State[CPU, Memory] => [[ 5.7780527 58.4338502]]\n",
      "Action:3, Reward:700\n",
      "episode: 726/1000\n",
      "State[CPU, Memory] => [[65.24438347 59.27047971]]\n",
      "Action:4, Reward:600\n",
      "episode: 727/1000\n",
      "State[CPU, Memory] => [[29.94077113  5.66609689]]\n",
      "Action:1, Reward:900\n",
      "episode: 728/1000\n",
      "State[CPU, Memory] => [[89.93803151 67.27509699]]\n",
      "Action:7, Reward:300\n",
      "episode: 729/1000\n",
      "State[CPU, Memory] => [[23.45459445 60.27561695]]\n",
      "Action:3, Reward:700\n",
      "episode: 730/1000\n",
      "State[CPU, Memory] => [[66.51866266 54.7050037 ]]\n",
      "Action:7, Reward:300\n",
      "episode: 731/1000\n",
      "State[CPU, Memory] => [[58.19637182 35.91599814]]\n",
      "Action:3, Reward:700\n",
      "episode: 732/1000\n",
      "State[CPU, Memory] => [[16.39648131 88.00686128]]\n",
      "Action:4, Reward:600\n",
      "episode: 733/1000\n",
      "State[CPU, Memory] => [[ 1.32574764 60.730081  ]]\n",
      "Action:6, Reward:400\n",
      "episode: 734/1000\n",
      "State[CPU, Memory] => [[50.96685159 34.72294796]]\n",
      "Action:4, Reward:600\n",
      "episode: 735/1000\n",
      "State[CPU, Memory] => [[ 8.75376362 70.68284199]]\n",
      "Action:4, Reward:600\n",
      "episode: 736/1000\n",
      "State[CPU, Memory] => [[85.52081315 80.0121275 ]]\n",
      "Action:6, Reward:400\n",
      "episode: 737/1000\n",
      "State[CPU, Memory] => [[36.63697605 73.7163164 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 738/1000\n",
      "State[CPU, Memory] => [[91.03525928 32.98899456]]\n",
      "Action:0, Reward:1000\n",
      "episode: 739/1000\n",
      "State[CPU, Memory] => [[73.73961787 19.10104972]]\n",
      "Action:1, Reward:900\n",
      "episode: 740/1000\n",
      "State[CPU, Memory] => [[80.03585518 11.4230391 ]]\n",
      "Action:6, Reward:400\n",
      "episode: 741/1000\n",
      "State[CPU, Memory] => [[97.85135465  6.5504083 ]]\n",
      "Action:3, Reward:700\n",
      "episode: 742/1000\n",
      "State[CPU, Memory] => [[20.36635188 24.27012086]]\n",
      "Action:3, Reward:700\n",
      "episode: 743/1000\n",
      "State[CPU, Memory] => [[54.51390503 10.68106476]]\n",
      "Action:0, Reward:1000\n",
      "episode: 744/1000\n",
      "State[CPU, Memory] => [[52.04009774 67.9555955 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 745/1000\n",
      "State[CPU, Memory] => [[12.91843689 76.15901394]]\n",
      "Action:7, Reward:300\n",
      "episode: 746/1000\n",
      "State[CPU, Memory] => [[ 1.0326505 46.3304735]]\n",
      "Action:6, Reward:400\n",
      "episode: 747/1000\n",
      "State[CPU, Memory] => [[44.66971509  9.3069383 ]]\n",
      "Action:5, Reward:500\n",
      "episode: 748/1000\n",
      "State[CPU, Memory] => [[12.07658599 58.16304617]]\n",
      "Action:0, Reward:1000\n",
      "episode: 749/1000\n",
      "State[CPU, Memory] => [[ 7.0937218  86.85365225]]\n",
      "Action:5, Reward:500\n",
      "episode: 750/1000\n",
      "State[CPU, Memory] => [[55.98538932 77.42246173]]\n",
      "Action:5, Reward:500\n",
      "episode: 751/1000\n",
      "State[CPU, Memory] => [[5.29644827e-02 8.05879351e+01]]\n",
      "Action:8, Reward:200\n",
      "episode: 752/1000\n",
      "State[CPU, Memory] => [[26.04160589 31.26741272]]\n",
      "Action:8, Reward:200\n",
      "episode: 753/1000\n",
      "State[CPU, Memory] => [[85.07946563 25.46177645]]\n",
      "Action:2, Reward:800\n",
      "episode: 754/1000\n",
      "State[CPU, Memory] => [[35.14279919 74.23234995]]\n",
      "Action:5, Reward:500\n",
      "episode: 755/1000\n",
      "State[CPU, Memory] => [[32.76875781 34.63902023]]\n",
      "Action:2, Reward:800\n",
      "episode: 756/1000\n",
      "State[CPU, Memory] => [[24.83722286 24.77767095]]\n",
      "Action:0, Reward:1000\n",
      "episode: 757/1000\n",
      "State[CPU, Memory] => [[41.82123032 98.34279215]]\n",
      "Action:0, Reward:1000\n",
      "episode: 758/1000\n",
      "State[CPU, Memory] => [[15.55707827 93.47832318]]\n",
      "Action:3, Reward:700\n",
      "episode: 759/1000\n",
      "State[CPU, Memory] => [[22.2835545  28.77343584]]\n",
      "Action:6, Reward:400\n",
      "episode: 760/1000\n",
      "State[CPU, Memory] => [[36.48072556 11.31182688]]\n",
      "Action:1, Reward:900\n",
      "episode: 761/1000\n",
      "State[CPU, Memory] => [[51.47175923 40.81375222]]\n",
      "Action:0, Reward:1000\n",
      "episode: 762/1000\n",
      "State[CPU, Memory] => [[91.59334546 19.71448837]]\n",
      "Action:6, Reward:400\n",
      "episode: 763/1000\n",
      "State[CPU, Memory] => [[10.85036369 26.91816818]]\n",
      "Action:5, Reward:500\n",
      "episode: 764/1000\n",
      "State[CPU, Memory] => [[49.56016185 77.00637715]]\n",
      "Action:2, Reward:800\n",
      "episode: 765/1000\n",
      "State[CPU, Memory] => [[62.17945779 15.78542818]]\n",
      "Action:6, Reward:400\n",
      "episode: 766/1000\n",
      "State[CPU, Memory] => [[54.30622073  0.68436308]]\n",
      "Action:8, Reward:200\n",
      "episode: 767/1000\n",
      "State[CPU, Memory] => [[88.33019061 76.74125457]]\n",
      "Action:8, Reward:200\n",
      "episode: 768/1000\n",
      "State[CPU, Memory] => [[ 1.41052719 57.61928308]]\n",
      "Action:7, Reward:300\n",
      "episode: 769/1000\n",
      "State[CPU, Memory] => [[74.15516922 38.9511133 ]]\n",
      "Action:7, Reward:300\n",
      "episode: 770/1000\n",
      "State[CPU, Memory] => [[ 4.17414376 48.14979671]]\n",
      "Action:6, Reward:400\n",
      "episode: 771/1000\n",
      "State[CPU, Memory] => [[64.93071502 20.75837574]]\n",
      "Action:0, Reward:1000\n",
      "episode: 772/1000\n",
      "State[CPU, Memory] => [[63.02682272 46.30596571]]\n",
      "Action:2, Reward:800\n",
      "episode: 773/1000\n",
      "State[CPU, Memory] => [[39.13117878 42.99710458]]\n",
      "Action:4, Reward:600\n",
      "episode: 774/1000\n",
      "State[CPU, Memory] => [[85.91615714 96.4776228 ]]\n",
      "Action:3, Reward:700\n",
      "episode: 775/1000\n",
      "State[CPU, Memory] => [[24.89961782 65.48349167]]\n",
      "Action:8, Reward:200\n",
      "episode: 776/1000\n",
      "State[CPU, Memory] => [[36.26214973 98.96920102]]\n",
      "Action:1, Reward:900\n",
      "episode: 777/1000\n",
      "State[CPU, Memory] => [[94.9684694  27.82442347]]\n",
      "Action:6, Reward:400\n",
      "episode: 778/1000\n",
      "State[CPU, Memory] => [[78.21803684 98.73431001]]\n",
      "Action:2, Reward:800\n",
      "episode: 779/1000\n",
      "State[CPU, Memory] => [[73.42544969 25.84391792]]\n",
      "Action:0, Reward:1000\n",
      "episode: 780/1000\n",
      "State[CPU, Memory] => [[54.77009264 74.45515333]]\n",
      "Action:3, Reward:700\n",
      "episode: 781/1000\n",
      "State[CPU, Memory] => [[27.70689878 54.5135137 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 782/1000\n",
      "State[CPU, Memory] => [[18.02147542 67.5273778 ]]\n",
      "Action:0, Reward:1000\n",
      "episode: 783/1000\n",
      "State[CPU, Memory] => [[27.56073732 16.21611325]]\n",
      "Action:7, Reward:300\n",
      "episode: 784/1000\n",
      "State[CPU, Memory] => [[97.69023909 67.49799324]]\n",
      "Action:3, Reward:700\n",
      "episode: 785/1000\n",
      "State[CPU, Memory] => [[43.6967199 67.3659035]]\n",
      "Action:6, Reward:400\n",
      "episode: 786/1000\n",
      "State[CPU, Memory] => [[22.5828666  46.09498914]]\n",
      "Action:8, Reward:200\n",
      "episode: 787/1000\n",
      "State[CPU, Memory] => [[12.09750068 79.77935767]]\n",
      "Action:7, Reward:300\n",
      "episode: 788/1000\n",
      "State[CPU, Memory] => [[14.75986454 70.80006037]]\n",
      "Action:0, Reward:1000\n",
      "episode: 789/1000\n",
      "State[CPU, Memory] => [[48.71730793 84.03100212]]\n",
      "Action:3, Reward:700\n",
      "episode: 790/1000\n",
      "State[CPU, Memory] => [[66.98916971 31.44039979]]\n",
      "Action:4, Reward:600\n",
      "episode: 791/1000\n",
      "State[CPU, Memory] => [[58.77705746  1.48101861]]\n",
      "Action:5, Reward:500\n",
      "episode: 792/1000\n",
      "State[CPU, Memory] => [[46.7135131  34.16532762]]\n",
      "Action:3, Reward:700\n",
      "episode: 793/1000\n",
      "State[CPU, Memory] => [[ 9.64207991 45.55222113]]\n",
      "Action:8, Reward:200\n",
      "episode: 794/1000\n",
      "State[CPU, Memory] => [[86.19771907 39.04304155]]\n",
      "Action:2, Reward:800\n",
      "episode: 795/1000\n",
      "State[CPU, Memory] => [[95.59806953 93.7660267 ]]\n",
      "Action:4, Reward:600\n",
      "episode: 796/1000\n",
      "State[CPU, Memory] => [[27.1703285   1.71320181]]\n",
      "Action:0, Reward:1000\n",
      "episode: 797/1000\n",
      "State[CPU, Memory] => [[45.68389365 92.39919715]]\n",
      "Action:5, Reward:500\n",
      "episode: 798/1000\n",
      "State[CPU, Memory] => [[99.79993943 12.76429438]]\n",
      "Action:0, Reward:1000\n",
      "episode: 799/1000\n",
      "State[CPU, Memory] => [[56.84164519  7.49772285]]\n",
      "Action:3, Reward:700\n",
      "episode: 800/1000\n",
      "State[CPU, Memory] => [[10.86609009 55.4979778 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 801/1000\n",
      "State[CPU, Memory] => [[11.57417203 27.4882574 ]]\n",
      "Action:4, Reward:600\n",
      "episode: 802/1000\n",
      "State[CPU, Memory] => [[89.06193776 75.17480733]]\n",
      "Action:1, Reward:900\n",
      "episode: 803/1000\n",
      "State[CPU, Memory] => [[14.46250371 26.59675395]]\n",
      "Action:4, Reward:600\n",
      "episode: 804/1000\n",
      "State[CPU, Memory] => [[33.41549647 89.31448722]]\n",
      "Action:4, Reward:600\n",
      "episode: 805/1000\n",
      "State[CPU, Memory] => [[25.32259375 17.52045839]]\n",
      "Action:8, Reward:200\n",
      "episode: 806/1000\n",
      "State[CPU, Memory] => [[87.54051422 90.92656197]]\n",
      "Action:5, Reward:500\n",
      "episode: 807/1000\n",
      "State[CPU, Memory] => [[11.53575734 39.08615241]]\n",
      "Action:3, Reward:700\n",
      "episode: 808/1000\n",
      "State[CPU, Memory] => [[34.54189381  9.81971083]]\n",
      "Action:4, Reward:600\n",
      "episode: 809/1000\n",
      "State[CPU, Memory] => [[52.22829075 54.25511616]]\n",
      "Action:7, Reward:300\n",
      "episode: 810/1000\n",
      "State[CPU, Memory] => [[11.19689727 85.70881103]]\n",
      "Action:1, Reward:900\n",
      "episode: 811/1000\n",
      "State[CPU, Memory] => [[92.92622074 19.83634516]]\n",
      "Action:4, Reward:600\n",
      "episode: 812/1000\n",
      "State[CPU, Memory] => [[57.00819991 60.71678257]]\n",
      "Action:1, Reward:900\n",
      "episode: 813/1000\n",
      "State[CPU, Memory] => [[81.15601229 60.1855142 ]]\n",
      "Action:4, Reward:600\n",
      "episode: 814/1000\n",
      "State[CPU, Memory] => [[89.25929457 72.07668421]]\n",
      "Action:1, Reward:900\n",
      "episode: 815/1000\n",
      "State[CPU, Memory] => [[44.48282396 28.64868912]]\n",
      "Action:5, Reward:500\n",
      "episode: 816/1000\n",
      "State[CPU, Memory] => [[52.97444826 16.76501516]]\n",
      "Action:4, Reward:600\n",
      "episode: 817/1000\n",
      "State[CPU, Memory] => [[39.08602246  6.71432401]]\n",
      "Action:1, Reward:900\n",
      "episode: 818/1000\n",
      "State[CPU, Memory] => [[89.45056954 54.62389691]]\n",
      "Action:1, Reward:900\n",
      "episode: 819/1000\n",
      "State[CPU, Memory] => [[92.97375457 75.29012747]]\n",
      "Action:5, Reward:500\n",
      "episode: 820/1000\n",
      "State[CPU, Memory] => [[90.61046114 89.59919299]]\n",
      "Action:1, Reward:900\n",
      "episode: 821/1000\n",
      "State[CPU, Memory] => [[ 0.80453897 28.00478831]]\n",
      "Action:3, Reward:700\n",
      "episode: 822/1000\n",
      "State[CPU, Memory] => [[8.71809819 2.10353009]]\n",
      "Action:0, Reward:1000\n",
      "episode: 823/1000\n",
      "State[CPU, Memory] => [[20.11174934 21.98859963]]\n",
      "Action:8, Reward:200\n",
      "episode: 824/1000\n",
      "State[CPU, Memory] => [[ 3.3887537  40.25433338]]\n",
      "Action:0, Reward:1000\n",
      "episode: 825/1000\n",
      "State[CPU, Memory] => [[97.623143   44.65483826]]\n",
      "Action:6, Reward:400\n",
      "episode: 826/1000\n",
      "State[CPU, Memory] => [[94.01801655 38.17508392]]\n",
      "Action:8, Reward:200\n",
      "episode: 827/1000\n",
      "State[CPU, Memory] => [[32.0057637  63.26224965]]\n",
      "Action:5, Reward:500\n",
      "episode: 828/1000\n",
      "State[CPU, Memory] => [[84.71681969 56.82233768]]\n",
      "Action:4, Reward:600\n",
      "episode: 829/1000\n",
      "State[CPU, Memory] => [[97.48118771 68.49943445]]\n",
      "Action:7, Reward:300\n",
      "episode: 830/1000\n",
      "State[CPU, Memory] => [[79.75290987  4.09681976]]\n",
      "Action:1, Reward:900\n",
      "episode: 831/1000\n",
      "State[CPU, Memory] => [[92.87435567 40.66534063]]\n",
      "Action:1, Reward:900\n",
      "episode: 832/1000\n",
      "State[CPU, Memory] => [[52.58508985 94.2797734 ]]\n",
      "Action:2, Reward:800\n",
      "episode: 833/1000\n",
      "State[CPU, Memory] => [[21.1142037  72.74834614]]\n",
      "Action:5, Reward:500\n",
      "episode: 834/1000\n",
      "State[CPU, Memory] => [[46.26813926  1.9932001 ]]\n",
      "Action:7, Reward:300\n",
      "episode: 835/1000\n",
      "State[CPU, Memory] => [[35.74925169 90.85581026]]\n",
      "Action:0, Reward:1000\n",
      "episode: 836/1000\n",
      "State[CPU, Memory] => [[11.65020237 82.21888719]]\n",
      "Action:8, Reward:200\n",
      "episode: 837/1000\n",
      "State[CPU, Memory] => [[63.89442237 18.80001591]]\n",
      "Action:4, Reward:600\n",
      "episode: 838/1000\n",
      "State[CPU, Memory] => [[83.39274259 99.44053344]]\n",
      "Action:7, Reward:300\n",
      "episode: 839/1000\n",
      "State[CPU, Memory] => [[44.27082148 74.4986589 ]]\n",
      "Action:5, Reward:500\n",
      "episode: 840/1000\n",
      "State[CPU, Memory] => [[55.2929177  22.09016608]]\n",
      "Action:8, Reward:200\n",
      "episode: 841/1000\n",
      "State[CPU, Memory] => [[24.27658734 77.32199451]]\n",
      "Action:2, Reward:800\n",
      "episode: 842/1000\n",
      "State[CPU, Memory] => [[62.5648946  17.95869589]]\n",
      "Action:5, Reward:500\n",
      "episode: 843/1000\n",
      "State[CPU, Memory] => [[92.98157786 39.11532021]]\n",
      "Action:7, Reward:300\n",
      "episode: 844/1000\n",
      "State[CPU, Memory] => [[97.30464588 78.12412673]]\n",
      "Action:3, Reward:700\n",
      "episode: 845/1000\n",
      "State[CPU, Memory] => [[ 2.95825023 30.03107306]]\n",
      "Action:1, Reward:900\n",
      "episode: 846/1000\n",
      "State[CPU, Memory] => [[27.75531332 74.67108049]]\n",
      "Action:3, Reward:700\n",
      "episode: 847/1000\n",
      "State[CPU, Memory] => [[63.80973927 10.29814797]]\n",
      "Action:5, Reward:500\n",
      "episode: 848/1000\n",
      "State[CPU, Memory] => [[91.4078281   6.81581685]]\n",
      "Action:2, Reward:800\n",
      "episode: 849/1000\n",
      "State[CPU, Memory] => [[ 3.70381938 18.85092896]]\n",
      "Action:1, Reward:900\n",
      "episode: 850/1000\n",
      "State[CPU, Memory] => [[22.4377883  45.47370086]]\n",
      "Action:6, Reward:400\n",
      "episode: 851/1000\n",
      "State[CPU, Memory] => [[35.21269584 51.76622713]]\n",
      "Action:5, Reward:500\n",
      "episode: 852/1000\n",
      "State[CPU, Memory] => [[42.44260072 83.04610119]]\n",
      "Action:7, Reward:300\n",
      "episode: 853/1000\n",
      "State[CPU, Memory] => [[47.00289013 81.16340811]]\n",
      "Action:5, Reward:500\n",
      "episode: 854/1000\n",
      "State[CPU, Memory] => [[95.20638714 55.9996822 ]]\n",
      "Action:4, Reward:600\n",
      "episode: 855/1000\n",
      "State[CPU, Memory] => [[49.7803149 12.7478308]]\n",
      "Action:4, Reward:600\n",
      "episode: 856/1000\n",
      "State[CPU, Memory] => [[ 3.57633902 49.12145344]]\n",
      "Action:2, Reward:800\n",
      "episode: 857/1000\n",
      "State[CPU, Memory] => [[83.14317081 89.85833474]]\n",
      "Action:6, Reward:400\n",
      "episode: 858/1000\n",
      "State[CPU, Memory] => [[96.89254976 12.15620572]]\n",
      "Action:3, Reward:700\n",
      "episode: 859/1000\n",
      "State[CPU, Memory] => [[33.67927387 87.93952812]]\n",
      "Action:7, Reward:300\n",
      "episode: 860/1000\n",
      "State[CPU, Memory] => [[84.69342233 58.43616852]]\n",
      "Action:1, Reward:900\n",
      "episode: 861/1000\n",
      "State[CPU, Memory] => [[16.47638013 44.67222566]]\n",
      "Action:4, Reward:600\n",
      "episode: 862/1000\n",
      "State[CPU, Memory] => [[29.76360097  4.27861506]]\n",
      "Action:8, Reward:200\n",
      "episode: 863/1000\n",
      "State[CPU, Memory] => [[97.82928614 26.67137453]]\n",
      "Action:7, Reward:300\n",
      "episode: 864/1000\n",
      "State[CPU, Memory] => [[45.65656874 93.35520329]]\n",
      "Action:1, Reward:900\n",
      "episode: 865/1000\n",
      "State[CPU, Memory] => [[ 9.00307015 95.84030375]]\n",
      "Action:1, Reward:900\n",
      "episode: 866/1000\n",
      "State[CPU, Memory] => [[42.85491204 32.96655045]]\n",
      "Action:0, Reward:1000\n",
      "episode: 867/1000\n",
      "State[CPU, Memory] => [[82.44613493  6.51319832]]\n",
      "Action:5, Reward:500\n",
      "episode: 868/1000\n",
      "State[CPU, Memory] => [[93.79932803 94.96312025]]\n",
      "Action:4, Reward:600\n",
      "episode: 869/1000\n",
      "State[CPU, Memory] => [[21.32607688 41.75049553]]\n",
      "Action:0, Reward:1000\n",
      "episode: 870/1000\n",
      "State[CPU, Memory] => [[90.39863101 11.35408018]]\n",
      "Action:0, Reward:1000\n",
      "episode: 871/1000\n",
      "State[CPU, Memory] => [[50.99785559 86.09146752]]\n",
      "Action:3, Reward:700\n",
      "episode: 872/1000\n",
      "State[CPU, Memory] => [[21.51591329 85.04139901]]\n",
      "Action:8, Reward:200\n",
      "episode: 873/1000\n",
      "State[CPU, Memory] => [[65.44811972 31.47081877]]\n",
      "Action:8, Reward:200\n",
      "episode: 874/1000\n",
      "State[CPU, Memory] => [[58.15802629 49.42586514]]\n",
      "Action:8, Reward:200\n",
      "episode: 875/1000\n",
      "State[CPU, Memory] => [[96.89486851 64.97965242]]\n",
      "Action:7, Reward:300\n",
      "episode: 876/1000\n",
      "State[CPU, Memory] => [[82.85105078 21.70392909]]\n",
      "Action:5, Reward:500\n",
      "episode: 877/1000\n",
      "State[CPU, Memory] => [[22.80873091 26.58062435]]\n",
      "Action:3, Reward:700\n",
      "episode: 878/1000\n",
      "State[CPU, Memory] => [[16.36103514 27.9157264 ]]\n",
      "Action:7, Reward:300\n",
      "episode: 879/1000\n",
      "State[CPU, Memory] => [[93.96686206 39.67126378]]\n",
      "Action:2, Reward:800\n",
      "episode: 880/1000\n",
      "State[CPU, Memory] => [[48.39606509 17.41959074]]\n",
      "Action:2, Reward:800\n",
      "episode: 881/1000\n",
      "State[CPU, Memory] => [[81.11411581 49.44349362]]\n",
      "Action:6, Reward:400\n",
      "episode: 882/1000\n",
      "State[CPU, Memory] => [[46.79196941 31.98667865]]\n",
      "Action:2, Reward:800\n",
      "episode: 883/1000\n",
      "State[CPU, Memory] => [[65.47881749 61.04411367]]\n",
      "Action:2, Reward:800\n",
      "episode: 884/1000\n",
      "State[CPU, Memory] => [[93.40614028  6.0040588 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 885/1000\n",
      "State[CPU, Memory] => [[73.24013277 26.13427576]]\n",
      "Action:6, Reward:400\n",
      "episode: 886/1000\n",
      "State[CPU, Memory] => [[27.7307497  29.93031601]]\n",
      "Action:0, Reward:1000\n",
      "episode: 887/1000\n",
      "State[CPU, Memory] => [[ 6.06516406 67.94047243]]\n",
      "Action:0, Reward:1000\n",
      "episode: 888/1000\n",
      "State[CPU, Memory] => [[98.2999836  74.25661204]]\n",
      "Action:7, Reward:300\n",
      "episode: 889/1000\n",
      "State[CPU, Memory] => [[ 3.74988624 20.11612603]]\n",
      "Action:8, Reward:200\n",
      "episode: 890/1000\n",
      "State[CPU, Memory] => [[36.29871236 88.20739609]]\n",
      "Action:3, Reward:700\n",
      "episode: 891/1000\n",
      "State[CPU, Memory] => [[88.33075989 35.35965265]]\n",
      "Action:4, Reward:600\n",
      "episode: 892/1000\n",
      "State[CPU, Memory] => [[83.53505036 31.04689604]]\n",
      "Action:1, Reward:900\n",
      "episode: 893/1000\n",
      "State[CPU, Memory] => [[82.43247149 53.55799513]]\n",
      "Action:0, Reward:1000\n",
      "episode: 894/1000\n",
      "State[CPU, Memory] => [[74.32049085 66.45542833]]\n",
      "Action:3, Reward:700\n",
      "episode: 895/1000\n",
      "State[CPU, Memory] => [[ 0.10116899 73.47457611]]\n",
      "Action:6, Reward:400\n",
      "episode: 896/1000\n",
      "State[CPU, Memory] => [[41.23676675 48.57840206]]\n",
      "Action:0, Reward:1000\n",
      "episode: 897/1000\n",
      "State[CPU, Memory] => [[48.05421848 55.67939137]]\n",
      "Action:2, Reward:800\n",
      "episode: 898/1000\n",
      "State[CPU, Memory] => [[38.35371896 15.99187661]]\n",
      "Action:3, Reward:700\n",
      "episode: 899/1000\n",
      "State[CPU, Memory] => [[12.26272165 16.58924528]]\n",
      "Action:4, Reward:600\n",
      "episode: 900/1000\n",
      "State[CPU, Memory] => [[48.48859257 13.91885591]]\n",
      "Action:4, Reward:600\n",
      "episode: 901/1000\n",
      "State[CPU, Memory] => [[62.40894135 10.03813205]]\n",
      "Action:3, Reward:700\n",
      "episode: 902/1000\n",
      "State[CPU, Memory] => [[65.97684173 25.27554583]]\n",
      "Action:6, Reward:400\n",
      "episode: 903/1000\n",
      "State[CPU, Memory] => [[70.6596407  23.87327558]]\n",
      "Action:4, Reward:600\n",
      "episode: 904/1000\n",
      "State[CPU, Memory] => [[57.38384119 76.24449429]]\n",
      "Action:8, Reward:200\n",
      "episode: 905/1000\n",
      "State[CPU, Memory] => [[2.43382814 9.40728459]]\n",
      "Action:7, Reward:300\n",
      "episode: 906/1000\n",
      "State[CPU, Memory] => [[19.88798483 39.29680843]]\n",
      "Action:0, Reward:1000\n",
      "episode: 907/1000\n",
      "State[CPU, Memory] => [[69.26230193 69.92194081]]\n",
      "Action:2, Reward:800\n",
      "episode: 908/1000\n",
      "State[CPU, Memory] => [[22.83526202 17.9245436 ]]\n",
      "Action:3, Reward:700\n",
      "episode: 909/1000\n",
      "State[CPU, Memory] => [[23.57092438 63.31040138]]\n",
      "Action:3, Reward:700\n",
      "episode: 910/1000\n",
      "State[CPU, Memory] => [[69.52531358  6.11152767]]\n",
      "Action:5, Reward:500\n",
      "episode: 911/1000\n",
      "State[CPU, Memory] => [[ 5.76892625 16.39638385]]\n",
      "Action:1, Reward:900\n",
      "episode: 912/1000\n",
      "State[CPU, Memory] => [[90.68999956 22.84014654]]\n",
      "Action:7, Reward:300\n",
      "episode: 913/1000\n",
      "State[CPU, Memory] => [[78.16780261 66.07071971]]\n",
      "Action:7, Reward:300\n",
      "episode: 914/1000\n",
      "State[CPU, Memory] => [[67.32412347 68.86762287]]\n",
      "Action:1, Reward:900\n",
      "episode: 915/1000\n",
      "State[CPU, Memory] => [[10.20436679 82.57391213]]\n",
      "Action:8, Reward:200\n",
      "episode: 916/1000\n",
      "State[CPU, Memory] => [[72.07046989 23.21931861]]\n",
      "Action:3, Reward:700\n",
      "episode: 917/1000\n",
      "State[CPU, Memory] => [[73.72875946 35.28520482]]\n",
      "Action:2, Reward:800\n",
      "episode: 918/1000\n",
      "State[CPU, Memory] => [[ 0.11782188 10.78859974]]\n",
      "Action:1, Reward:900\n",
      "episode: 919/1000\n",
      "State[CPU, Memory] => [[52.32517553 98.74803608]]\n",
      "Action:7, Reward:300\n",
      "episode: 920/1000\n",
      "State[CPU, Memory] => [[93.57468814 15.03486565]]\n",
      "Action:7, Reward:300\n",
      "episode: 921/1000\n",
      "State[CPU, Memory] => [[48.79514168 55.70898298]]\n",
      "Action:2, Reward:800\n",
      "episode: 922/1000\n",
      "State[CPU, Memory] => [[39.88181133 82.87887507]]\n",
      "Action:2, Reward:800\n",
      "episode: 923/1000\n",
      "State[CPU, Memory] => [[39.18869512 23.20024162]]\n",
      "Action:8, Reward:200\n",
      "episode: 924/1000\n",
      "State[CPU, Memory] => [[64.31289404 34.42803423]]\n",
      "Action:2, Reward:800\n",
      "episode: 925/1000\n",
      "State[CPU, Memory] => [[15.08657734  9.10461677]]\n",
      "Action:4, Reward:600\n",
      "episode: 926/1000\n",
      "State[CPU, Memory] => [[82.99276414 14.65892782]]\n",
      "Action:3, Reward:700\n",
      "episode: 927/1000\n",
      "State[CPU, Memory] => [[ 2.87442716 14.16145065]]\n",
      "Action:5, Reward:500\n",
      "episode: 928/1000\n",
      "State[CPU, Memory] => [[75.77583292 10.41242747]]\n",
      "Action:8, Reward:200\n",
      "episode: 929/1000\n",
      "State[CPU, Memory] => [[55.39845171 39.06885207]]\n",
      "Action:7, Reward:300\n",
      "episode: 930/1000\n",
      "State[CPU, Memory] => [[90.10576167 52.41537461]]\n",
      "Action:6, Reward:400\n",
      "episode: 931/1000\n",
      "State[CPU, Memory] => [[15.965383   11.52927081]]\n",
      "Action:1, Reward:900\n",
      "episode: 932/1000\n",
      "State[CPU, Memory] => [[52.4467469  51.79344184]]\n",
      "Action:0, Reward:1000\n",
      "episode: 933/1000\n",
      "State[CPU, Memory] => [[88.91461462 52.43895159]]\n",
      "Action:2, Reward:800\n",
      "episode: 934/1000\n",
      "State[CPU, Memory] => [[85.00919321 51.43255053]]\n",
      "Action:0, Reward:1000\n",
      "episode: 935/1000\n",
      "State[CPU, Memory] => [[19.36612833 91.80553835]]\n",
      "Action:1, Reward:900\n",
      "episode: 936/1000\n",
      "State[CPU, Memory] => [[37.6616602  18.20328435]]\n",
      "Action:0, Reward:1000\n",
      "episode: 937/1000\n",
      "State[CPU, Memory] => [[14.10923852 14.59103233]]\n",
      "Action:0, Reward:1000\n",
      "episode: 938/1000\n",
      "State[CPU, Memory] => [[ 7.01529057 29.83425939]]\n",
      "Action:1, Reward:900\n",
      "episode: 939/1000\n",
      "State[CPU, Memory] => [[23.02755537 48.84809954]]\n",
      "Action:8, Reward:200\n",
      "episode: 940/1000\n",
      "State[CPU, Memory] => [[43.95605983 73.62662578]]\n",
      "Action:6, Reward:400\n",
      "episode: 941/1000\n",
      "State[CPU, Memory] => [[34.33301758 32.89880299]]\n",
      "Action:3, Reward:700\n",
      "episode: 942/1000\n",
      "State[CPU, Memory] => [[52.31602407 31.6971916 ]]\n",
      "Action:2, Reward:800\n",
      "episode: 943/1000\n",
      "State[CPU, Memory] => [[56.3103816 61.7740625]]\n",
      "Action:7, Reward:300\n",
      "episode: 944/1000\n",
      "State[CPU, Memory] => [[30.69357541 91.69153847]]\n",
      "Action:6, Reward:400\n",
      "episode: 945/1000\n",
      "State[CPU, Memory] => [[57.98937378 52.9857037 ]]\n",
      "Action:2, Reward:800\n",
      "episode: 946/1000\n",
      "State[CPU, Memory] => [[30.32863763 37.51559503]]\n",
      "Action:3, Reward:700\n",
      "episode: 947/1000\n",
      "State[CPU, Memory] => [[70.93227385 16.0137167 ]]\n",
      "Action:6, Reward:400\n",
      "episode: 948/1000\n",
      "State[CPU, Memory] => [[32.64863547 49.83431563]]\n",
      "Action:0, Reward:1000\n",
      "episode: 949/1000\n",
      "State[CPU, Memory] => [[19.79113577  4.80222654]]\n",
      "Action:2, Reward:800\n",
      "episode: 950/1000\n",
      "State[CPU, Memory] => [[88.14889374 95.69000673]]\n",
      "Action:3, Reward:700\n",
      "episode: 951/1000\n",
      "State[CPU, Memory] => [[ 9.80637537 72.4412628 ]]\n",
      "Action:0, Reward:1000\n",
      "episode: 952/1000\n",
      "State[CPU, Memory] => [[72.39938478 37.1750103 ]]\n",
      "Action:1, Reward:900\n",
      "episode: 953/1000\n",
      "State[CPU, Memory] => [[67.26798709 70.12251517]]\n",
      "Action:5, Reward:500\n",
      "episode: 954/1000\n",
      "State[CPU, Memory] => [[73.00887216 87.71887323]]\n",
      "Action:6, Reward:400\n",
      "episode: 955/1000\n",
      "State[CPU, Memory] => [[33.89758976 40.29247334]]\n",
      "Action:1, Reward:900\n",
      "episode: 956/1000\n",
      "State[CPU, Memory] => [[12.64677601 80.46456711]]\n",
      "Action:2, Reward:800\n",
      "episode: 957/1000\n",
      "State[CPU, Memory] => [[62.23627806 81.76160369]]\n",
      "Action:5, Reward:500\n",
      "episode: 958/1000\n",
      "State[CPU, Memory] => [[59.06463851 93.95695126]]\n",
      "Action:4, Reward:600\n",
      "episode: 959/1000\n",
      "State[CPU, Memory] => [[12.70059088 41.92838353]]\n",
      "Action:1, Reward:900\n",
      "episode: 960/1000\n",
      "State[CPU, Memory] => [[11.73496778 24.47195467]]\n",
      "Action:8, Reward:200\n",
      "episode: 961/1000\n",
      "State[CPU, Memory] => [[93.98504014 54.77194739]]\n",
      "Action:1, Reward:900\n",
      "episode: 962/1000\n",
      "State[CPU, Memory] => [[ 6.00312636 53.11093539]]\n",
      "Action:6, Reward:400\n",
      "episode: 963/1000\n",
      "State[CPU, Memory] => [[20.05768302 92.02549575]]\n",
      "Action:0, Reward:1000\n",
      "episode: 964/1000\n",
      "State[CPU, Memory] => [[39.98678294 57.21427171]]\n",
      "Action:7, Reward:300\n",
      "episode: 965/1000\n",
      "State[CPU, Memory] => [[95.35309212 29.94541152]]\n",
      "Action:1, Reward:900\n",
      "episode: 966/1000\n",
      "State[CPU, Memory] => [[79.94555567 77.2508935 ]]\n",
      "Action:4, Reward:600\n",
      "episode: 967/1000\n",
      "State[CPU, Memory] => [[94.50047369 21.46314786]]\n",
      "Action:3, Reward:700\n",
      "episode: 968/1000\n",
      "State[CPU, Memory] => [[99.85607661 33.43725512]]\n",
      "Action:0, Reward:1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 969/1000\n",
      "State[CPU, Memory] => [[26.42074847 99.66675315]]\n",
      "Action:3, Reward:700\n",
      "episode: 970/1000\n",
      "State[CPU, Memory] => [[50.27957732 14.73572232]]\n",
      "Action:4, Reward:600\n",
      "episode: 971/1000\n",
      "State[CPU, Memory] => [[72.95249809 21.43627327]]\n",
      "Action:0, Reward:1000\n",
      "episode: 972/1000\n",
      "State[CPU, Memory] => [[22.51056834 86.38094905]]\n",
      "Action:2, Reward:800\n",
      "episode: 973/1000\n",
      "State[CPU, Memory] => [[49.70654063 33.20880256]]\n",
      "Action:5, Reward:500\n",
      "episode: 974/1000\n",
      "State[CPU, Memory] => [[59.71864232 10.3822512 ]]\n",
      "Action:8, Reward:200\n",
      "episode: 975/1000\n",
      "State[CPU, Memory] => [[23.63127402 42.76442777]]\n",
      "Action:2, Reward:800\n",
      "episode: 976/1000\n",
      "State[CPU, Memory] => [[44.41727397  5.73562231]]\n",
      "Action:8, Reward:200\n",
      "episode: 977/1000\n",
      "State[CPU, Memory] => [[ 7.94569947 30.13671026]]\n",
      "Action:1, Reward:900\n",
      "episode: 978/1000\n",
      "State[CPU, Memory] => [[33.12591311 16.71781865]]\n",
      "Action:3, Reward:700\n",
      "episode: 979/1000\n",
      "State[CPU, Memory] => [[59.94610853 47.00272967]]\n",
      "Action:6, Reward:400\n",
      "episode: 980/1000\n",
      "State[CPU, Memory] => [[36.61224784 12.81100051]]\n",
      "Action:5, Reward:500\n",
      "episode: 981/1000\n",
      "State[CPU, Memory] => [[80.45692691 79.88384434]]\n",
      "Action:4, Reward:600\n",
      "episode: 982/1000\n",
      "State[CPU, Memory] => [[66.63027348 94.27162823]]\n",
      "Action:6, Reward:400\n",
      "episode: 983/1000\n",
      "State[CPU, Memory] => [[61.6232035  94.00184691]]\n",
      "Action:2, Reward:800\n",
      "episode: 984/1000\n",
      "State[CPU, Memory] => [[28.63115608  0.84361563]]\n",
      "Action:6, Reward:400\n",
      "episode: 985/1000\n",
      "State[CPU, Memory] => [[36.24622541 20.66624244]]\n",
      "Action:4, Reward:600\n",
      "episode: 986/1000\n",
      "State[CPU, Memory] => [[ 5.24205898 85.45825369]]\n",
      "Action:6, Reward:400\n",
      "episode: 987/1000\n",
      "State[CPU, Memory] => [[24.70793599 20.80385805]]\n",
      "Action:3, Reward:700\n",
      "episode: 988/1000\n",
      "State[CPU, Memory] => [[92.22414967 34.62245444]]\n",
      "Action:5, Reward:500\n",
      "episode: 989/1000\n",
      "State[CPU, Memory] => [[63.00002781  4.8184297 ]]\n",
      "Action:4, Reward:600\n",
      "episode: 990/1000\n",
      "State[CPU, Memory] => [[ 8.86865041 39.35667349]]\n",
      "Action:0, Reward:1000\n",
      "episode: 991/1000\n",
      "State[CPU, Memory] => [[46.6035406  97.87914675]]\n",
      "Action:7, Reward:300\n",
      "episode: 992/1000\n",
      "State[CPU, Memory] => [[88.14537963 35.81348276]]\n",
      "Action:5, Reward:500\n",
      "episode: 993/1000\n",
      "State[CPU, Memory] => [[72.48481833 10.24430578]]\n",
      "Action:8, Reward:200\n",
      "episode: 994/1000\n",
      "State[CPU, Memory] => [[69.09163794  3.79997775]]\n",
      "Action:5, Reward:500\n",
      "episode: 995/1000\n",
      "State[CPU, Memory] => [[43.49826271 38.49912588]]\n",
      "Action:2, Reward:800\n",
      "episode: 996/1000\n",
      "State[CPU, Memory] => [[82.65878481 61.79940911]]\n",
      "Action:6, Reward:400\n",
      "episode: 997/1000\n",
      "State[CPU, Memory] => [[96.8211147  68.84657031]]\n",
      "Action:5, Reward:500\n",
      "episode: 998/1000\n",
      "State[CPU, Memory] => [[29.66378666 28.43848771]]\n",
      "Action:5, Reward:500\n",
      "episode: 999/1000\n",
      "State[CPU, Memory] => [[12.96073739  0.39772222]]\n",
      "Action:8, Reward:200\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "for e in range(n_episodes): # iterate over new episodes of the game\n",
    "    print(\"episode: {}/{}\".format(e,n_episodes))\n",
    "    \n",
    "    state = env.reset() # reset state at start of each new episode of the game    \n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    print(\"State[CPU, Memory] => {}\".format(state))\n",
    "    \n",
    "    action = agent.act(state) # action is to select to migrate out some of the VMs from 0 to 8\n",
    "    reward = env.reward_out[action]\n",
    "    print(\"Action:{}, Reward:{}\".format(action,reward))\n",
    "    \n",
    "#     next_state, reward, done, _ = env.step(action) # agent interacts with env, gets feedback; 4 state data points, e.g., pole angle, cart position        \n",
    "#     reward = reward if not done else -10 # reward +1 for each additional frame with pole upright        \n",
    "#     next_state = np.reshape(next_state, [1, state_size])\n",
    "#     agent.remember(state, action, reward, next_state, done) # remember the previous timestep's state, actions, reward, etc.        \n",
    "#     state = next_state # set \"current state\" for upcoming iteration to the current next state \n",
    "    \n",
    "#     for time in range(5000):  # time represents a frame of the game; goal is to keep pole upright as long as possible up to range, e.g., 500 or 5000 timesteps\n",
    "#         env.render()\n",
    "#         action = agent.act(state) # action is either 0 or 1 (move cart left or right); decide on one or other here\n",
    "#         next_state, reward, done, _ = env.step(action) # agent interacts with env, gets feedback; 4 state data points, e.g., pole angle, cart position        \n",
    "#         reward = reward if not done else -10 # reward +1 for each additional frame with pole upright        \n",
    "#         next_state = np.reshape(next_state, [1, state_size])\n",
    "#         agent.remember(state, action, reward, next_state, done) # remember the previous timestep's state, actions, reward, etc.        \n",
    "#         state = next_state # set \"current state\" for upcoming iteration to the current next state        \n",
    "#         if done: # episode ends if agent drops pole or we reach timestep 5000\n",
    "#             print(\"episode: {}/{}, score: {}, e: {:.2}\" # print the episode's score and agent's epsilon\n",
    "#                   .format(e, n_episodes, time, agent.epsilon))\n",
    "#             break # exit loop\n",
    "#     if len(agent.memory) > batch_size:\n",
    "#         agent.replay(batch_size) # train the agent by replaying the experiences of the episode\n",
    "    if e % 50 == 0:\n",
    "        agent.save(output_dir + \"weights_\" + '{:04d}'.format(e) + \".hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
